{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b56a466",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a6a1e",
   "metadata": {},
   "source": [
    "# Assessment: Virtual Assistant with Riva Speech AI and Question Answering\n",
    "\n",
    "Question answering (QA) tasks consists of generating an answer, given a natural language query and a context (knowledge content).\n",
    "\n",
    "- **Extractive QA**: Predict the span within the context with a start and end position which indicates the answer to the question.\n",
    "- **Generative QA**: Generate a natural answer for the query with no constraint that the answer should be a span within the context.\n",
    "\n",
    "Riva supports out-of-the-box extractive QA with a BERT model.\n",
    "\n",
    "<img src=\"images/assess/bert_QA.png\" width=500>\n",
    "\n",
    "### Table of Contents\n",
    "[The Problem](#The-Problem)<br>\n",
    "[Scoring](#Scoring)<br>\n",
    "[Step 1: Launch Riva Server](#Step-1:-Launch-Riva-Server)<br>\n",
    "[Step 2: ASR Query](#Step-2:-ASR-Query)<br>\n",
    "[Step 3: ASR Customization](#Step-3:-ASR-Customization)<br>\n",
    "[Step 4: Question Answering](#Step-4:-Question-Answering)<br>\n",
    "[Step 5: TTS Query](#Step-5:-TTS-Query)<br>\n",
    "[Step 6: Simple Virtual Assistant](#Step-6:-Simple-Virtual-Assistant)<br>\n",
    "[Step 7: Submit Your Assessment](#Step-7:-Submit-Your-Assessment)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e9575",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "To successfully run this notebook, be sure you have:\n",
    "\n",
    "1. **NGC Credentials**<br>Be sure you have added your NGC credential as described in the [NGC Setup notebook](003_Intro_NGC_Setup.ipynb).  If you have restarted the course instance, you will need to repeat this step.\n",
    "\n",
    "2. **Killed all Docker containers**<br>Run the following cell to make sure all containers are shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh...\n",
    "# Clear Docker containers\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e9af4-793c-4f81-a4e3-055011d85de4",
   "metadata": {},
   "source": [
    "3. **Cleared GPU Memory**<br>\n",
    "Make sure you have shut down all other notebooks to fully clear GPU memory.  Verify this is the case by running the following cell and observing the Memory Usage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854c92a-bbb9-4bf2-a047-bfedd04efe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d134ae",
   "metadata": {},
   "source": [
    "---\n",
    "# The Problem\n",
    "In this assessment, you'll build a virtual assistant application that integrates ASR, TTS, and NLP services to answer questions about EMEA (Europe, Middle East, and Africa).  Your virtual assistant should be able to answer an audio question about EMEA and respond in speech with the answer to the question!  \n",
    "\n",
    "<img src=\"images/assess/IVA_QA.png\">\n",
    "\n",
    "You'll need to add some customizations along the way to make this work properly for EMEA. In summary, you'll:\n",
    "- Launch Riva ASR, TTS, and NLP (includes QA) services\n",
    "- Customize the virtual assistant transcriber to recognize \"EMEA\"\n",
    "- Build a simple dialog manager (DM) with extractive QA from external documents about EMEA\n",
    "- Customize the virtual assistant pronunciation of \"EMEA\"\n",
    "- Run the virtual assistant end-to-end for a complete conversational AI dialog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14abce",
   "metadata": {},
   "source": [
    "---\n",
    "# Scoring\n",
    "You will be assessed on your ability to effectively and efficiently build and deploy the application.  This coding assessment is worth 70 points, divided as follows:\n",
    "\n",
    "\n",
    "| Step                         | Graded                                                    | FIXMEs?  | Points |\n",
    "|------------------------------|-----------------------------------------------------------|----------|--------|\n",
    "| 1. Launch Riva               | Riva Server (correct config; models run)                  |    4     |   12   |\n",
    "| 2. ASR Query            | Request for transcription (check ASR config )                  |    3     |    9   |\n",
    "| 3. ASR Customization    | Improve transcription with ASR customization (evaluate error)  |    3     |   15   |\n",
    "| 4. Question Answering   | Request Q&A on new context (check returned answer)             |    2     |   10   |\n",
    "| 5. TTS Query            | Request for TTS (check TTS config )                            |    3     |    9   |\n",
    "| 6. Simple Virtual Assistant  | Put all together (check answer)                           |    3     |   15   |\n",
    "\n",
    "\n",
    "Although you are very capable at this point of building the project without any help at all, some scaffolding is provided, including specific names for variables and files.  This is for the benefit of the autograder, so please use these constructs for your assessment.  In addition, output for your executed cells is periodically saved in the `my_assessment` directory for grading.  Along the way, there are a few opportunities to check your work to see if you are on the right track. \n",
    "\n",
    "Once you are confident that you've built a reliable virtual assistant, follow the instructions for submission at the end of the notebook.\n",
    "\n",
    "### Resources and Hints\n",
    "\n",
    "* **[Riva Speech Skills User's Guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/index.html)**<br>\n",
    "* **Riva Deployment Examples**<br>\n",
    "Review what you've learned in the [ASR Deployment](005_ASR_Deployment.ipynb) and [TTS Deployment](007_TTS_Deployment.ipynb) notebooks about how to deploy and query Riva ASR/TTS services.  \n",
    "* **Riva Customization Examples**<br>\n",
    "Review what you've learned in the [Full Pipeline](008_Full_Pipeline.ipynb) notebook about building a simple dialog manager and customizing Riva ASR/TTS services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ff466",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 1: Launch Riva Server\n",
    "### Set Up Project Paths, Libraries, and Models (not graded)\n",
    "For the full pipeline, we'll need to deploy default models for ASR, TTS, and NLP services. The `riva_init.sh` command loads and builds models specific to the GPU you are using, but to save time for this course, these have been preloaded.\n",
    "\n",
    "The next few cells create some useful path names and copy all of the optimized models into `/dli_workspace/riva-assessment-model-repo` for convenience.  This is the repo you must use for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefef670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory and model repo\n",
    "WORKSPACE='/dli_workspace'\n",
    "RIVA_QS = WORKSPACE + \"/riva_quickstart\"\n",
    "RIVA_MODEL_REPO = WORKSPACE + \"/riva-assessment-model-repo\"\n",
    "!mkdir -p $RIVA_MODEL_REPO\n",
    "\n",
    "# load required libraries\n",
    "import riva.client\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import io\n",
    "import time\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ca08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Copy all the ASR, TTS, and NLP models for convenience (faster deployment)\n",
    "# Time is about 1-2 minutes for the copy\n",
    "cp -rn  /dli_workspace/riva-asr-model-repo/* \\\n",
    "    /dli_workspace/riva-assessment-model-repo/\n",
    "\n",
    "cp -rn  /dli_workspace/riva-tts-model-repo/* \\\n",
    "    /dli_workspace/riva-assessment-model-repo/\n",
    "\n",
    "cp -rn  /dli_workspace/riva-full-model-repo/* \\\n",
    "    /dli_workspace/riva-assessment-model-repo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17484c89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check to see what models are there now\n",
    "!ls $RIVA_MODEL_REPO/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd9d8c",
   "metadata": {},
   "source": [
    "### Configure Riva (graded)\n",
    "\n",
    "Open [config.sh](dli_workspace/riva_quickstart/config.sh) and modify it to deploy all three services (ASR, NLP, TTS) with the out-of-the-box default models, specifying the newly created model repository created above for this assessment. Save your work.\n",
    "```\n",
    " service_enabled_asr=FIXME\n",
    " service_enabled_nlp=FIXME\n",
    " service_enabled_tts=FIXME\n",
    " riva_model_loc=FIXME\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc5b7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your work - are all three services enabled? Is the model location repo correct? \n",
    "! cat dli_workspace/riva_quickstart/config.sh \\\n",
    "   2>&1|tee my_assessment/step1.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385f387",
   "metadata": {},
   "source": [
    "### Launch Riva Server (not graded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Riva server (about 1 minute)\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720d7f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker logs riva-speech "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c9044",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: ASR Query\n",
    "### Listen to the Samples (not graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_1=\"/dli_workspace/data/usa4.wav\"\n",
    "SAMPLE_2=\"/dli_workspace/data/emea_assess_q2_resampled.wav\"\n",
    "\n",
    "print(\"Sample 1\")\n",
    "ipd.display(ipd.Audio(SAMPLE_1))\n",
    "\n",
    "print(\"Sample 2\")\n",
    "ipd.display(ipd.Audio(SAMPLE_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72d546",
   "metadata": {},
   "source": [
    "### Request ASR Transcription (graded)\n",
    "Build a function that creates and executes a transcription request from Riva. In the API request, specify:\n",
    "* English language\n",
    "* Maximum alternatives = 2\n",
    "* Punctuation enabled (true)\n",
    "\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) and run the next two cells to load the function into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac846a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the functions and FIXMEs\n",
    "def asr_predict(SAMPLE):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    riva_asr = riva.client.ASRService(auth)\n",
    "    asr_config = riva.client.RecognitionConfig()\n",
    "    #FIXME                   # set the language code to English\n",
    "    #FIXME                   # set max alternatives to two\n",
    "    #FIXME                   # set punctuation to true\n",
    "    with io.open(SAMPLE, 'rb') as fh:\n",
    "        content = fh.read()\n",
    "    response = riva_asr.offline_recognize(content, asr_config)\n",
    "    transcript=response.results[0].alternatives[0].transcript\n",
    "    return transcript, asr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6618d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work.\n",
    "# Did the function provide the expected transcription?\n",
    "transcript, asr_config = asr_predict(SAMPLE_1)\n",
    "print(\"ASR Transcript SAMPLE_1:\", transcript)\n",
    "\n",
    "# DO NOT REMOVE\n",
    "with open('/dli/task/my_assessment/step2.txt', 'w') as f:\n",
    "    f.write(str(asr_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the 2nd sample\n",
    "transcript, asr_config=asr_predict(SAMPLE_2)\n",
    "print(\"ASR Transcript SAMPLE_2:\", transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94efec9",
   "metadata": {},
   "source": [
    "Transcribing EMEA is not accurate due to the fact that there is no such term in the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db8cc3",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 3: ASR Customization\n",
    "In order to provide a correct transcription for the SAMPLE_2, please add the pronunciation for \"EMEA\" to the lexicon.\n",
    "\n",
    "### Check the lexicon for the word \"emea\" (not graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe30731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CONFORMER_OFFLINE = \"conformer-en-US-asr-offline-ctc-decoder-cpu-streaming-offline\"\n",
    "LEXICON = os.path.join(RIVA_MODEL_REPO, \"models\", CONFORMER_OFFLINE, \"1\", \"lexicon.txt\")\n",
    "\n",
    "! grep \"^emea\\b\" $LEXICON "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7b694",
   "metadata": {},
   "source": [
    "### Add \"emea\" to the lexicon (graded)\n",
    "1. Stop Riva\n",
    "1. Find the tokenizer location\n",
    "1. Get new encodings for EMEA with `sentencepiece` and the tokenizer\n",
    "1. Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) to replace the value in the lexicon \n",
    "1. Restart Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Riva\n",
    "! bash $RIVA_QS/riva_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the tokenizer\n",
    "import glob\n",
    "import sentencepiece as spm\n",
    "\n",
    "# locate the _tokenizer.model in Riva models repo\n",
    "mydir = os.path.join(RIVA_MODEL_REPO, \"models\", CONFORMER_OFFLINE, \"1\")\n",
    "os.chdir(mydir)\n",
    "for file in glob.glob(\"*.model\"):\n",
    "    filename = file\n",
    "    \n",
    "tokenizer = os.path.join(RIVA_MODEL_REPO, \"models\", CONFORMER_OFFLINE, \"1\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete FIXMEs\n",
    "# Set the token and pronuciation \"emea\" and transcribe it\n",
    "# The token is the correct spelling; the pronunciation is simply \"emea\"\n",
    "\n",
    "TOKEN=#FIXME\n",
    "PRONUNCIATION=#FIXME\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file=tokenizer)\n",
    "print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=False, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the FIXME\n",
    "# Add the encoded line you produced to the lexicon\n",
    "\n",
    "! echo -e \"FIXME\" >> $LEXICON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2433fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "# Was a new entry for \"emea\" added to the lexicon?\n",
    "! grep \"^emea\\b\" $LEXICON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Riva\n",
    "! bash $RIVA_QS/riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work.  \n",
    "# Is the transcription for \"emea\" spelled correctly? (don't worry about capitalization)\n",
    "transcript, asr_config=asr_predict(SAMPLE_2)\n",
    "print(\"ASR Transcript:\", transcript)\n",
    "\n",
    "# DO NOT REMOVE\n",
    "with open('/dli/task/my_assessment/step3.txt', 'w') as f:\n",
    "    f.write(str(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb59a9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### _Troubleshooting Step 3_\n",
    "_Note: A malformed lexicon entry may keep Riva from starting.  If you've \"messed up\" the lexicon, you can restore it to its original state by uncommenting the following cell and executing it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP_LEXICON = os.path.join(\"/dli_workspace/riva-asr-model-repo\", \"models\", CONFORMER_OFFLINE, \"1\", \"lexicon.txt\")\n",
    "# !cp $BACKUP_LEXICON $LEXICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74401418",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 4: Question Answering\n",
    "In this step, you'll first explore the QA model using an example about the USA.  Next, you'll apply what you've learned to adapt your QA to the EMEA use case. \n",
    "\n",
    "The inputs to extractive QA:\n",
    "* query - question asked\n",
    "* context - the text with the information\n",
    "\n",
    "The output from extractive QA:\n",
    "* response - answer extracted from the context, or blank if not available\n",
    " \n",
    "Define the queries and context files you will need for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries and content files needed for the examples\n",
    "USA_CONTENT_FILE='/dli_workspace/data/usa.txt'\n",
    "EMEA_CONTENT_FILE='/dli_workspace/data/emea.txt'\n",
    "qa_query1 = \"How many states in the US?\"\n",
    "qa_query2 = \"What is the capital of USA?\"\n",
    "qa_query3 = \"What does EMEA stand for?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0d4a6",
   "metadata": {},
   "source": [
    "### USA Example (not graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80015050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a QA response dialog manager\n",
    "def dm_predict(qa_query, q_context):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    nlp_service = riva.client.NLPService(auth)\n",
    "    response = nlp_service.natural_query(qa_query, q_context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ba7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display USA Context\n",
    "with open(USA_CONTENT_FILE) as f:\n",
    "    usa_context = f.readlines()\n",
    "usa_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA example 1\n",
    "response1 = dm_predict(qa_query1, usa_context[0])\n",
    "\n",
    "print(\"Question: \", qa_query1)\n",
    "print(\"Answer: \", response1.results[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA example 2\n",
    "response2 = dm_predict(qa_query2, usa_context[0])\n",
    "\n",
    "print(\"Question: \", qa_query2)\n",
    "print(\"Answer: \", response2.results[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA example 3\n",
    "response3 = dm_predict(qa_query3, usa_context[0])\n",
    "\n",
    "print(\"Question: \", qa_query3)\n",
    "print(\"Answer: \", response3.results[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f628c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if response is empty\n",
    "if response3.results[0].answer=='' :\n",
    "    print(\"No response to '{}'\".format(qa_query3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ceee6",
   "metadata": {},
   "source": [
    "### Load the EMEA Context (graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59a0af",
   "metadata": {},
   "source": [
    "`qa_query3` had no answer in the USA example, as expected.  This is because the USA context is focused on USA knowledge, not EMEA knowledge. Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) in the following cell and try again with EMEA knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06177f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the FIXMEs\n",
    "# Check your work.  Did you get a valid answer to your question?\n",
    "\n",
    "# Create the context\n",
    "with open(#FIXME) as f:\n",
    "    emea_context = f.readlines()\n",
    "\n",
    "# Query the model with the correct context\n",
    "response3 = dm_predict(qa_query3, #FIXME)\n",
    "                       \n",
    "print(\"Question: \", qa_query3)\n",
    "print(\"Answer: \", response3.results[0].answer)\n",
    "\n",
    "# DO NOT REMOVE\n",
    "with open('/dli/task/my_assessment/step4.txt', 'w') as f:\n",
    "    f.write(str(response3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7399ce",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 5: TTS Query\n",
    "\n",
    "### Request TTS Speech (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) in the following cell to configure TTS with:\n",
    "- English language \n",
    "- Correct sample_rate_hz\n",
    "- Male voice_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the FIXMEs for the correct configuration\n",
    "\n",
    "sample_rate_hz = #FIXME\n",
    "\n",
    "# helper function for more readable output\n",
    "def remove_braces(braced_text):\n",
    "    return braced_text.replace(\"{@\",\"\").replace(\"}\",\"\")\n",
    "\n",
    "# Define a Python function to create speech from text\n",
    "def tts_predict(text):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    riva_tts = riva.client.SpeechSynthesisService(auth)\n",
    "    req = { \n",
    "            \"language_code\"  : #FIXME,\n",
    "            \"sample_rate_hz\" : sample_rate_hz,                 \n",
    "            \"voice_name\"     : #FIXME                    \n",
    "    }\n",
    "    req[\"text\"] = text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    return audio_samples, remove_braces(resp.meta.processed_text), req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work.\n",
    "# Is the speech output correct?\n",
    "\n",
    "req=[]\n",
    "audio_samples, processed_text, req =tts_predict(response3.results[0].answer)\n",
    "\n",
    "# DO NOT REMOVE\n",
    "with open('/dli/task/my_assessment/step5.txt', 'w') as f:\n",
    "    f.write(str(req))\n",
    "\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1960f3",
   "metadata": {},
   "source": [
    "### Customize EMEA TTS Pronunciation (not graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084aaf1d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import AlignerModel\n",
    "aligner = AlignerModel.from_pretrained(\"tts_en_radtts_aligner_ipa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b727eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some IPA options for EMEA \n",
    "# em'ea\n",
    "# emea\n",
    "# 'EM ˈmiə'\n",
    "# EM MIA\n",
    "# emia\n",
    "\n",
    "input_string = \"emia\"\n",
    "text_g2p = aligner.tokenizer.g2p(input_string)\n",
    "print(text_g2p)\n",
    "text_tokens = aligner.tokenizer(input_string)\n",
    "print(text_tokens)\n",
    "print(\"\\n\" + ''.join(text_g2p))\n",
    "synth_audio, processed_text, req  =tts_predict(''.join(text_g2p))\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz))\n",
    "EMEA_IPA=\"\".join(text_g2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"What does EMEA stand for?\"\n",
    "req=[]\n",
    "C=''.join(text_g2p)\n",
    "audio_samples, processed_text, req =tts_predict(text.replace(\"EMEA\",EMEA_IPA))\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ce463",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 6: Simple Virtual Assistant\n",
    "Time to put the whole application together!\n",
    "\n",
    "### Complete the Virtual Assistant Application (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) in the following cell and run the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all together\n",
    "\n",
    "SAMPLE=\"/dli_workspace/data/e_mea_assess_resampled.wav\"\n",
    "\n",
    "print(\"First Audio sample:\")\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "\n",
    "# call Riva ASR  \n",
    "transcript, asr_config=#FIXME\n",
    "print(transcript)\n",
    "\n",
    "\n",
    "# call Dialog Manager\n",
    "dm_response = #FIXME\n",
    "\n",
    "# call Riva TTS\n",
    "synth_audio, processed_text, req =#FIXME\n",
    "\n",
    "# DO NOT REMOVE\n",
    "with open('/dli/task/my_assessment/step6.txt', 'w') as f:\n",
    "    assessment_responses = [transcript, dm_response, processed_text]\n",
    "    f.write(str(assessment_responses))\n",
    "\n",
    "time.sleep(3)\n",
    "print(\"Virtual Assistant Response:\")\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b67582",
   "metadata": {},
   "source": [
    "### Stop Riva Services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f88410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Riva \n",
    "!bash $RIVA_QS/riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d73dd",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 7: Submit Your Assessment\n",
    "How were your results? \n",
    "\n",
    "If you are satisfied that you have completed the code correctly, and that your virtual assistant is correct, you can submit your project as follows to the autograder:\n",
    "\n",
    "1. Go back to the GPU launch page and click the checkmark to run the assessment:\n",
    "\n",
    "<img src=\"images/assess/assessment_checkmark.png\">\n",
    "\n",
    "2. That's it!  You'll receive your grade feedback in the pop-up window similar to the example below:\n",
    "\n",
    "<img src=\"images/assess/assessment_pass_popup.png\">\n",
    "\n",
    "You can check your assessment progress in the course progress tab.  Note that partial values for the coding assessment **won't be visible here - it shows up as either 0 (if you achieve <65) or the full 70 points**.  Be sure to complete the additional questions to qualify for your final certificate!\n",
    "\n",
    "<img src=\"images/assess/progress.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540144c0",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
