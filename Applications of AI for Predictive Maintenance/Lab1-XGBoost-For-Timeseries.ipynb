{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Header\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Predictive Maintenance using NVIDIA RAPIDS and Deep Learning Models</h1>\n",
    "<h4 align=\"center\">Part 1: Training GPU XGBoost models with RAPIDS for Time Series</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with time series algorithms can be difficult to configure and apply to arbitrary sequence prediction problems, even with well-defined and “easy to use” interfaces like those provided in the tf.keras deep learning library in Python.  One reason for this difficulty is the need for Deep Learning layers to input and return sequences rather than single values.\n",
    "\n",
    "In this course, we will take a publicly available time series dataset of hard drive operational metrics provided by <a href=\"https://www.backblaze.com/b2/hard-drive-test-data.html\">Backblaze</a>, to build out and compare machine learning vs deep learning approaches to predictive maintenance problems. \n",
    "\n",
    "The course materials are categorized in three sections. In this lab, we take a machine learning approach called “XGBoost” for classification of faulty/normal hard drives and discuss performance optimizations by utilizing accelerated computing. In lab 2, we move on to deep learning models and start with recurrent networks to predict the time series. We further explore variations of recurrent networks such as CNN-LSTM models. Lab 3, addresses applications of autoencoders to classify hard drives into faulty and normal bins. Throughout these labs, you will have the opportunity to get familiarized with details of implementations for each model, and learn how to curate the data and evaluate results. To begin, we start with \"Training GPU XGBoost models with RAPIDS for Time Series\". This lab covers the following topics:\n",
    "\n",
    "* [Background](#1)\n",
    "* [Predictive Maintenance: Predicting Anomalies Accurately and Early](#2)\n",
    "* [Lab Overview](#3)\n",
    "* [Dataset Loading and Pre-Processing](#4)\n",
    "* [Training GPU XGBoost models with RAPIDS for Time Series](#5)\n",
    "* [XGBoost](#6)\n",
    "* [More on Data Preparation](#7)\n",
    "    * [Exercise 1: Remove \"zero-valued\" columns](#e1)\n",
    "* [Define XGBoost Model Parameters](#8)\n",
    "* [Train XGBoost on CPUs](#9)\n",
    "* [Training with GPU without cuDF DataFrame](#10)\n",
    "* [Training with GPU and cuDF](#11)\n",
    "* [XGBoost Model Accuracy](#12)\n",
    "* [Measuring Model Accuracy](#13)\n",
    "* [Interpreting the results](#14)\n",
    "* [Dealing with Imbalanced Data](#15)\n",
    "    * [Exercise 2: Implications of balancing data samples](#e2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Background\n",
    "\n",
    "One of the important recent topics of interest for industrial organizations  is to move away from reactive and schedule-based maintenance to <b>predictive maintenance</b> that helps optimize the operations and improve productivity.\n",
    "\n",
    "<a name=\"2\"></a>\n",
    "### Predictive Maintenance: Predicting Anomalies Accurately and Early\n",
    "\n",
    "Whereas schedule-based maintenance provides a regularly scheduled time for part replacements and repairs, predictive maintenance represents a condition based approach driven by data and analytics. Predictive maintenance strives to identify anomalies that can lead to costly breakdowns, such as improper machinery lubrication, contamination, misalignment, or suboptimal humidity and temperature conditions.\n",
    "\n",
    "By identifying such anomalies of many part failures, estimating remaining useful life of these parts and then mapping anomalies to failure conditions by interpreting service logs, predictive maintenance helps to manage failures and avoid costly unplanned downtime. Other benefits include fewer unnecessary repairs, optimized spare parts inventory, and longer lifespans of equipment and parts—ultimately increasing equipment availability while improving operational efficiency. \n",
    "\n",
    "In this regard, our goal in this lab is to show you how you can take your company's time series data and leverage it to predict outcomes.\n",
    "\n",
    "<a name=\"3\"></a>\n",
    "## Lab Overview\n",
    "\n",
    "One of the common problems in a datacenter is failing data storage disks. Current business applications transfer large amounts of data and put very big pressure on the disks.  Using hard drive S.M.A.R.T. (Self-Monitoring Analysis and Reporting Technology) data provided from Backblaze, we will build conventional Machine Learning models in addition to Deep Learning methods to predict RUL (Remaining Useful Life) of a hard drive.\n",
    "\n",
    "- Dataset - https://www.backblaze.com/b2/hard-drive-test-data.html\n",
    "- Research paper from IBM - https://researcher.watson.ibm.com/researcher/files/us-mqiao/BigDataCongress_2018.pdf\n",
    "\n",
    "After completing this Lab, you will know how to take time series data and:\n",
    "\n",
    "- Predict Failure using a machine learning classification model with XGBoost\n",
    "- Predict Failure using a deep learning classification model using LSTM\n",
    "- Detect Anomalies using an Autoencoder (AE) or Seq2Seq model\n",
    "\n",
    "\n",
    "Let's start by importing some libraries used throughout this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that will be needed for the Lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from random import shuffle, randrange\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## Dataset Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of a daily snapshot of each operational hard drive in the Backblaze datacenter at the time of measurement.  Each snapshot includes basic drive information along with S.M.A.R.T. statistics reported by that drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define location of our input and prepared data files\n",
    "data_dir = './data/'\n",
    "\n",
    "csv_train_file = 'Lab1-2017-Full_data.csv.gz'\n",
    "csv_test_file = 'Lab1-2016-Q4_data.csv.gz'\n",
    "\n",
    "pkl_train_file = 'Lab1-2017-Full-ST4000DM000.pkl'\n",
    "pkl_test_file = 'Lab1-2016-Q4-ST4000DM000.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read entire SMART training data set\n",
    "# - Full 2017 dataset takes about 4 minutes to read in\n",
    "\n",
    "print('Reading training data set...')\n",
    "df = pd.read_csv(data_dir + csv_train_file)\n",
    "print('Finished reading training data set')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Overview<br>\n",
    "\n",
    "The dataset in this DLI course is provided by Backblaze. More documentation can be found [here](https://www.backblaze.com/b2/hard-drive-test-data.html) about the type of the data and the different sensors which indicate the health of the hard drives. Following is a list of key parameters reported in the dataset. \n",
    "\n",
    "- <b>Date:</b> The date of the entrie recording in yyyy-mm-dd format.\n",
    "\n",
    "- <b>Serial Number:</b> The manufacturer-assigned serial number of the drive.\n",
    "\n",
    "- <b>Model:</b> The manufacturer-assigned model number of the drive.\n",
    "\n",
    "- <b>Capacity:</b> The drive capacity in bytes.\n",
    "\n",
    "- <b>Failure:</b> Contains a “0” if the drive is OK. Contains a “1” if this is the last day the drive was operational before failing.\n",
    "\n",
    "- <b>SMART Stats:</b> – 90 columns of data, which are the Raw and Normalized values for 45 different SMART stats as reported by the given drive. Each value is the number reported by the drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the snapshot above, one important thing to notice is that we have several different models for which data is recorded. First, let's take a look at number of samples for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's review the available hard drive models and select one with lot of samples for this exercise\n",
    "print(df.model.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because SMART stats usage can vary in meaning from manufacturer to manufacturer, we will limit our training data to a specific hard drive model, to reduce  potential of other manufactures' usage from influencing our model. We will pick the `ST4000DM000` model, which has the largest number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST4000DM000 is the most popular drive so let's use this to train\n",
    "harddrive_model = 'ST4000DM000'\n",
    "\n",
    "# keep only the selected hard drive model we want in our training data set\n",
    "df = df[df.model == harddrive_model]\n",
    "\n",
    "# Load test set from Q2 data and pre-selected model\n",
    "print('Reading test data set...')\n",
    "df_t = pd.read_csv(data_dir + csv_test_file)\n",
    "print('Finished reading test data set')\n",
    "\n",
    "# keep only the selected hard drive model we want in our test data set\n",
    "df_t = df_t[df_t.model == harddrive_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting string to date time format for sorting and plotting time series data \n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df_t['date'] = pd.to_datetime(df_t['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "def plot_df(df,loc_col_list,axs):   \n",
    "    df_normal = df[df['failure']==0]\n",
    "    df_failed = df[df['failure']==1]\n",
    "    \n",
    "    n_rows = len(loc_col_list)\n",
    "    cmap = get_cmap(n_rows) \n",
    "    color_list = random.sample(range(n_rows),n_rows)\n",
    "    \n",
    "    for i, cur_col in enumerate(loc_col_list):\n",
    "        cur_min = df[cur_col].min() \n",
    "        cur_max = df[cur_col].max()\n",
    "        \n",
    "        axs[i].plot(df_normal['date'], df_normal[cur_col],'.', color=cmap(color_list[i]), markersize=5,alpha=0.2)\n",
    "        axs[i].plot(df_failed['date'], df_failed[cur_col],'.', color=cmap(color_list[i]), markersize=20,alpha=0.6)\n",
    "        axs[i].set_ylabel(cur_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_values(df,check_str): \n",
    "    col_list = df.columns.tolist()\n",
    "    loc_col_list = [] \n",
    "    \n",
    "    print(\"Range for    : {:>25s} {:>15s}\".format(\"min\",\"max\"))\n",
    "    \n",
    "    for cur_col in col_list:\n",
    "        if check_str in cur_col:\n",
    "            cur_min = df[cur_col].min() \n",
    "            cur_max = df[cur_col].max()\n",
    "\n",
    "            if cur_min != cur_max : \n",
    "                print(\"  {:20s}   {:15d} {:15d} \".format(cur_col,int(cur_min),int(cur_max)))\n",
    "                loc_col_list.append(cur_col)\n",
    "                \n",
    "    print(\"\")\n",
    "    return loc_col_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and understanding input data \n",
    "While building any machine learning/deep learning model, it is important to understand the quality and structure of the input data. For example, there can be missing data for several days for some of the sensors. There can also be sensors which do not have any data. The input data needs to be curated to remove such sensors and adjust the data for the sensors with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's remove columns that have no valid data\n",
    "df = df.dropna(axis='columns', how='all') \n",
    "# Next, let's remove a few rows that had invalid data (most likely error reading SMART statistics)\n",
    "df = df.dropna(axis='rows', how='any')\n",
    "\n",
    "df_failed = df[df['failure']==1]\n",
    "df_normal = df[df['failure']==0]\n",
    "\n",
    "col_list_raw = remove_constant_values(df_failed,\"_raw\")\n",
    "col_list_normalized = remove_constant_values(df_failed,\"_normalized\")\n",
    "col_list = ['date', 'serial_number', 'model', 'capacity_bytes', 'failure'] \n",
    "col_list = col_list + col_list_raw + col_list_normalized\n",
    "\n",
    "df = df[col_list]\n",
    "\n",
    "#keep the same columns as the training dataframe \n",
    "df_t = df_t[col_list]\n",
    "\n",
    "#fill in the small number of null values in remaining columns \n",
    "df = df.fillna(0)\n",
    "df_t = df_t.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting the number of failed hard disks to compare the distributions of the sensor data with normal disks and also to plot time series data \n",
    "\n",
    "serial_num_list_failed = df_failed.serial_number.value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of sensor data \n",
    "Once the data is curated, a typical histogram plot of the data can reveal key operating points, outliers and provide further suggestions for preparing the data. The raw SMART sensor data is plotted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 \n",
    "m = len(col_list_raw)//n \n",
    "if len(col_list_raw)%n > 0 : m += 1 \n",
    "    \n",
    "fig, axs = plt.subplots(figsize=(20,20), ncols=n, nrows=m)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i,cur_col in enumerate(col_list_raw): \n",
    "    axs[i].hist(df_normal[cur_col],alpha=0.2,color=\"tab:blue\")\n",
    "    axs[i].hist(df_failed[cur_col],alpha=0.6,color=\"tab:orange\")\n",
    "    axs[i].set_yscale('log')\n",
    "    axs[i].set_title(cur_col)\n",
    "\n",
    "for i in range(len(col_list_raw),axs.shape[0]): \n",
    "    axs[i].axis('off')\n",
    "\n",
    "i = len(col_list_raw)\n",
    "axs[i].plot([],[],alpha=0.2,color=\"tab:blue\",label=\"Normal disks\",linewidth=15)\n",
    "axs[i].plot([],[],alpha=0.6,color=\"tab:orange\",label=\"Failed disks\",linewidth=15)\n",
    "axs[i].legend(fontsize=16)\n",
    "    \n",
    "len(col_list_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series visualization \n",
    "\n",
    "Of all the disks with available data, several disks fail. Since the SMART sensor data is time series, it is useful to visualize the sensor behavior for the failed disks. In the plots below, the raw sensor data are displayed for 10 failed disks out of the 1061 failed disks. This can give us insights to the behavior of the disk until the time of failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "n_rows = len(col_list_raw)\n",
    "fig, axs = plt.subplots(ncols=1, nrows=n_rows, \n",
    "                            figsize=(10,1.5*n_rows),sharex=True)\n",
    "for cur_num in serial_num_list_failed[:10]: # [:1]: \n",
    "    plot_df(df[df['serial_number']==cur_num],col_list_raw,axs)\n",
    "\n",
    "print(\"NOTE : \\n    The large circles represent time of failure.\\n    The lines show the data leading up to the point of faiilure. \\n    Only serial numbers of failed disks are being used.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets for future use\n",
    "print('Saving the train and test data sets...')\n",
    "df.to_pickle(data_dir+pkl_train_file)\n",
    "df_t.to_pickle(data_dir+pkl_test_file)\n",
    "print('Completed saving data sets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## Training GPU XGBoost models with RAPIDS for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will leverage the Backblaze Hard Drive SMART data to train an XGBoost model that will predict potential future failures.\n",
    "\n",
    "XGBoost will use the historical collection of SMART data to learn common feature patterns present in disk failures.  The model we create will be able to predict a failure based on the characteristics learned. To implement our XGBoost model we will be using  NVIDIA's [RAPIDS](https://rapids.ai/) which is an open source, GPU accelerated, data science platform.  RAPIDS includes cuML which is a \"scikit-learn-like\" library that contains GPU-accelerated versions of some of the most popular machine learning algorithms.\n",
    "\n",
    "In this lab, we will be using RAPIDS to train XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "### XGBoost\n",
    "\n",
    "XGBoost is widely known today as a \"go to\" model when working with structured data. In fact, there is a disproportionately large number of XGBoost-based winning entries of Kaggle competitions. In this lab, we introduce the model, but we do not intend to go through a rigorous mathematical formulation of XGBoost. For curious readers, we recommend reviewing of the original paper on [XGBoost](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)\n",
    "\n",
    "Traditionally, in tree-based ensemble methods such as Random Forests, we train each tree independently and the predictions of multiple trees are summed to obtain the final score. The figure below classifies whether someone would like to play computer games or not. The left and right trees are classifying gamers based on different features, namely age and whether samples use computers on a daily basis. A weighted combination of such trees would result in a more accurate decision.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/twocart.png\" style=\"margin-top:10px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 1. Tree classifier to detect computer gamers<br/>\n",
    "<i>Image credit: XGBoost: A Scalable Tree Boosting System, T. Chen and C. Guestrin</i></p>\n",
    "\n",
    "\n",
    "\n",
    "In the case of XGBoost, an implementation of Gradient Boosted Decision Trees, we repeatedly build new models and combine them into an ensemble. Unlike Random Forests, we build trees one at a time, where each new tree helps to correct errors made by previously trained tree. The incremental training procedure is shown in figure 2.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/Ensemble Tree.jpg\" style=\"margin-top:10px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 2. XGBoost training pipeline</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The loss function controls the predictive power of the model, and the XGBoost regularization term ensures simplicity and manages overfitting. Since \"Boosting\" focuses iteratively building learners for the most difficult parts of your data, it provides an efficient algorithm to deal with unbalanced datasets by strengthening the impact of the positive class. With this brief introduction to the method, we are going to further curate the data and implement the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## More on Data Preparation\n",
    "\n",
    "Now that we have filtered the dataset based on the `model` column, this column is not required anymore and can be removed. Also, the `date` and `serial_number` do not possess any useful information for our training. Later in the course, where we treat the hard drive date as time sequences, we would need to further sort the data using the `date` column. Let's start by removing these unnecessary columns at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAPIDS\n",
    "import cudf \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneccessary columns\n",
    "print(\"Dropping unneeded columns...\")\n",
    "df = df.drop(columns=['model', 'date', 'serial_number'])\n",
    "df_t = df_t.drop(columns=['model', 'date', 'serial_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training/testing our model, we require labels. The `failure` column provides these class labels. We need to extract these labels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Separating Training Data Features and Labels\")\n",
    "# separate training set features from labels\n",
    "df_train_target = pd.DataFrame(df['failure'])\n",
    "\n",
    "print(\"Separating Test Data Features and Labels\")\n",
    "# separate test set features from labels\n",
    "df_test_target = pd.DataFrame(df_t['failure'])\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each SMART measure on the list of columns has a companion \"normalized\" twin column. The values of the normalized column are just the scaled version of the \"raw\" columns. We can choose to opt either of these columns for this lab, as XGBoost is not sensitive to the scale of the values of each column. Here, we choose to keep the \"raw\" version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df.columns if c.lower().find(\"normalized\")==-1]\n",
    "df=df[cols]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"e1\"></a>\n",
    "## Exercise 1: Remove \"zero-valued\" rows\n",
    "\n",
    "Another important drawback with the data is \"zero-valued\" rows. These rows have zero values for every entry of the dataset and do not possess any useful information for training the model. We can remove these rows from the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove zero valued rows\n",
    "df = <<TO DO>>\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, click [here](#a1) for an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these columns are removed, we should apply the same to our test set. Note that for our test set, even if they contain non-zero values, since they are removed from the train set, they will not be useful. Also, we need to remove the `failure` column - which is our class labels! - from the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df_t.columns if c.lower() in df.columns]\n",
    "df_t=df_t[cols]\n",
    "\n",
    "df_train = df.drop(columns=['failure'])\n",
    "df_test = df_t.drop(columns=['failure'])\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few records from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## Define XGBoost Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to other machine learning models, there are hyperparameters to be set for the XGBoost. Below is the list of parameters of the model:\n",
    "\n",
    "- <b>max_tree_depth</b>:  Defines the maximum depth of the tree.  Deeper trees are more complex and able to learn more relations amongst the data, however, also tend to over-fit as the relations may be specific to the training set.\n",
    "\n",
    "- <b>tree_method</b>:  Used to control GPU vs CPU operation.  'hist' = CPU histogram method.  'gpu_hist' = GPU histogram method.\n",
    "\n",
    "- <b>subsample</b>:  Denotes the fraction of observations to be randomly sampled for each tree.  Lower values make the algorithm more conservative and prevent overfitting however, too small of a value might lead to underfitting.\n",
    "\n",
    "- <b>regularization</b>:  Applies an L1-regularization term to the weights.  Increasing this value makes the model more conservative and can speed up performance on very high-dimensional data sets.\n",
    "\n",
    "- <b>gamma</b>:  Gamma specifies the minimum loss reduction required to split a node.\n",
    "\n",
    "- <b>pos_weight</b>:  Controls the balance of positive and negative weights to deal with unbalanced classes.\n",
    "\n",
    "- <b>early_stop</b>:  Used to control over-fitting.  Attempts to find the inflection point when the performance on the test set starts to decrease while the performance on the training set continues to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TREE_DEPTH = 8\n",
    "TREE_METHOD = 'hist'\n",
    "ITERATIONS = 85\n",
    "SUBSAMPLE = 0.6\n",
    "REGULARIZATION = 1.3\n",
    "GAMMA = 0.3\n",
    "POS_WEIGHT = 1\n",
    "EARLY_STOP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## Train XGBoost on CPUs\n",
    "\n",
    "To demonstrate the benefits of GPU computing and the RAPIDS platform, we will first train a XGBoost model on the CPUs in our training enviroment. Let's take a look at the specs of the CPUs running on your cloud instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information on the CPUs\n",
    "!lscpu | grep 'Model name:'\n",
    "!lscpu | grep 'CPU(s)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert our data into XGBoost DMatrix format. DMatrix is optimized for memory efficiency and training speed in XGBoost. Next, we need to create a dictionary of hyperparameters discussed above and finally call the `train` method to start the training process. We also measure the total time to compare it with the GPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Converting data sets into XGBoost DMatrix format...')\n",
    "xgtrain = xgb.DMatrix(df_train, df_train_target)\n",
    "xgeval = xgb.DMatrix(df_test, df_test_target)\n",
    "print('Completed converting data sets')\n",
    "\n",
    "params = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "print('Starting CPU XGBoost Training...')\n",
    "bst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_cpu = time.time() - start_time\n",
    "print('CPU XGBoost training compeleted - elapsed time:', timetaken_cpu,'seconds')\n",
    "\n",
    "# free up memory\n",
    "del xgtrain\n",
    "del xgeval\n",
    "del bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"10\"></a>\n",
    "## Training with GPU without cuDF DataFrame\n",
    "\n",
    "We will now leverage GPUs to accelerate the XGBoost Training and compare the results to the CPU version.\n",
    "\n",
    "First, let's see which GPU we have in our lab system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure the XGBoost training algorithm to use GPUs, we set params[tree_method] = 'gpu_hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU, without using cuDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('Converting data sets into XGBoost DMatrix format...')\n",
    "xgtrain = xgb.DMatrix(df_train, df_train_target)\n",
    "xgeval = xgb.DMatrix(df_test, df_test_target)\n",
    "print('Completed converting data sets')\n",
    "\n",
    "params = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "print('Starting GPU XGBoost Training...')\n",
    "bst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_gpu_nocudf = time.time() - start_time\n",
    "print('GPU XGBoost training compeleted - elapsed time:', timetaken_gpu_nocudf,'seconds')\n",
    "\n",
    "# free up memory\n",
    "del xgtrain\n",
    "del xgeval\n",
    "del bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"11\"></a>\n",
    "## Training with GPU and cuDF\n",
    "\n",
    "In this section we will be using cuDF instead of pandas DataFrame. cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data, which is based on Apache Arrow columnar memory format. cuDF provides a pandas-like API for familiarity which allows us to easily accelerate workflows without going into the details of CUDA programming. For more information on cuDF, please refer to [RAPIDS Github Repositor](https://github.com/rapidsai/cudf).\n",
    "To use cuDF with the XGBoost algorithm, we load the Pandas Dataframe into a cuDF Dataframe (Python object type cudf.dataframe.dataframe.DataFrame) first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into cuDF Dataframe\n",
    "\n",
    "gdf_train = cudf.DataFrame.from_pandas(df_train)\n",
    "gdf_train_target = cudf.DataFrame.from_pandas(df_train_target)\n",
    "\n",
    "gdf_eval = cudf.DataFrame.from_pandas(df_test)\n",
    "gdf_eval_target = cudf.DataFrame.from_pandas(df_test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to measure the XGBoost training time using the created cuDF and on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU, with cuDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('Converting cuDF data sets into XGBoost DMatrix format...')\n",
    "xgtrain = xgb.DMatrix(gdf_train, gdf_train_target)\n",
    "xgeval = xgb.DMatrix(gdf_eval, gdf_eval_target)\n",
    "print('Completed converting data sets')\n",
    "\n",
    "params = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "print('Starting GPU XGBoost Training with cuDF Dataframes...')\n",
    "bst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_gpu = time.time() - start_time\n",
    "print('GPU XGBoost Training with cuDF Dataframes compeleted - elapsed time:', timetaken_gpu,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training time for the three configurations described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'CPU', 'GPU', 'GPU+cuDF'\n",
    "sizes = [timetaken_cpu, timetaken_gpu_nocudf, timetaken_gpu]\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(labels)]\n",
    "plt.bar(x_pos, sizes)\n",
    "plt.xlabel(\"XGBoost Version\")\n",
    "plt.ylabel(\"Time (secs)\")\n",
    "plt.title(\"Training Time\")\n",
    "plt.xticks(x_pos, labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"CPU Time Taken:\\n\", round(timetaken_cpu,1),\"seconds\")\n",
    "print(\"\\nGPU (no cuDF) Time Taken:\\n\", round(timetaken_gpu_nocudf,1),\"seconds\")\n",
    "print(\"\\nGPU (cuDF) Time Taken:\\n\", round(timetaken_gpu,1),\"seconds\")\n",
    "print(\"\\nTotal speed-up with RAPIDS:\\n\", round(timetaken_cpu/timetaken_gpu*100,1), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using the GPU + cuDF we see a significant speed-up. In the next section, we are going to explore the accuracy of the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"12\"></a>\n",
    "### XGBoost Model Accuracy\n",
    "\n",
    "Let's look at the model's accuracy on the evalutation set that we will use to compare against our Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting XGBoost prediction on test dataset...')\n",
    "\n",
    "# Use trained model to predict on test data set\n",
    "preds = bst.predict(xgeval)\n",
    "\n",
    "# Convert predictions to \"normal\" (0) or \"failed\"\n",
    "y_pred = []\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "for pred in preds:\n",
    "    if pred<=THRESHOLD:\n",
    "        y_pred.append(0)\n",
    "    if pred>THRESHOLD:\n",
    "        y_pred.append(1)\n",
    "\n",
    "# Output array of classifications        \n",
    "y_pred = np.asarray(y_pred)\n",
    "        \n",
    "y_true = df_test_target.values.reshape(len(preds))\n",
    "\n",
    "print('XGBoost prediction completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (Eval)\", round(accuracy_score(y_true, y_pred),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"13\"></a>\n",
    "## Measuring Model Accuracy\n",
    "\n",
    "In this section, we are going to plot the confusion matrix. While accuracy rate provides us with an overall outlook on how well our model is performing, we need more insights into the accuracy. Especially, since our dataset is small, and we need to compromise some aspects of accuracy rate in favor of others. The `classification_report` provides us with the accuracy stats broken down into `precision`, `recall`, `f1-score` and `support`. Before introducing these measures, let's define some abbreviations:\n",
    "\n",
    "For brevity we are using the following abbreviations: \n",
    "\n",
    "\n",
    "- __True Positive (TP)__:  test correctly classifies the input to the class (hard drive failed, and we classified it as fail)\n",
    "- __False Positive (FP)__:  test incorrectly classifies the input to the class (hard drive is normal, but we classified it as fail)\n",
    "- __False Negative (FN)__:  test incorrectly misses to classify the input to the class (hard drive failed but we classified as normal)\n",
    "- __True Negative (TN)__:  test correctly misses to classify the input to the class (hard drive is normal, and we classified as normal)\n",
    "\n",
    "The first measure is focused on identifying positive cases and is called __recall__. We define recall as the ability of the model to identify all true positive samples of the dataset. In mathematical terms, recall is the ratio of true positives over true positives plus false negatives. By other means, recall tells us, among all the test samples belonging to the output class, how many of them are identified correctly by the model:\n",
    "\n",
    "\\begin{equation*}\n",
    "recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation*}\n",
    "\n",
    "The next measure, is called __precision__ and is the ability of the model to identify the relevant samples only, and is defined as the ratio of true positives over true positives plus false positives:\n",
    "\n",
    "\\begin{equation*}\n",
    "precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Selecting a proper threshold, usually stems from a good balance between the precision and recall values. A well-known measure that provides such a balance is `F1 score`, which is a harmonic mean of precision and recall, and defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "{F_1 \\: score} = 2*\\frac{precision*recall}{precision+recall}\n",
    "\\end{equation*}\n",
    "\n",
    "F1 Score is generally considered a better overall measurement than accuracy when an uneven class distribution exists (large number of True Negatives) as in the case of predicting a hard drive failure.\n",
    "\n",
    "One very important measure is the recall rate on defective samples. We are interested in models that retrieve as many defective hard drives as possible. We try to predict at least half of the defective hard drives. Let's measure how our model is performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"normal\", \"fail\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"14\"></a>\n",
    "## Interpreting the results\n",
    "\n",
    "When trying to predict hard drive failure, we can interpret the classification report above in this way:\n",
    "\n",
    "- Precision is the percentage of times the model is correct when predicting the given class.\n",
    "- Recall is the percentage of times we predicted that class against the total number of instances of the class in the data set.\n",
    "\n",
    "Therefore, the classification report above is telling us that the model predicted a failure for ~67% of the 234 fail sequences, and was correct 3% of the time. As for normal hard drives the recall rate is 1.00 and we are accurate ~100% of the time. \n",
    "\n",
    "<b>note</b>:  your precision / recall percentages may vary slightly depending on the weights learned during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Group Discussion</b>\n",
    "\n",
    "What do the results above tell us and any ideas how to improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary of training data:')\n",
    "train_all_count = df.shape[0]\n",
    "train_fail_count = df[df['failure'] >= 1].shape[0]\n",
    "print ('- Number of disks : ', train_all_count)\n",
    "print ('- Number of failed disks: ', train_fail_count)\n",
    "print ('- Percentage of failed disks: %.4f' %(train_fail_count/train_all_count*100),'%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary of test data:')\n",
    "test_all_count = df_t.shape[0]\n",
    "test_fail_count = df_t[df_t['failure'] >= 1].shape[0]\n",
    "print ('- Number of disks : ', test_all_count)\n",
    "print ('- Number of failed disks: ', test_fail_count)\n",
    "print ('- Percentage of failed disks: %.4f' %(test_fail_count/test_all_count*100),'%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and test sets have large class imbalances between normal and failed disks.  During the training process, our model is trying to find the optimal function for the lowest error loss.  Naively, if the model classifies every hard drive as normal, then the model is right more than 99% of the time.\n",
    "\n",
    "Thus, we need to come up with a strategy to address this imbalance. Some strategies to resolve class imbalance data include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Up / Down Sample Classes\n",
    "- Weighted Class Cost Optimization\n",
    "- Evaluate various error metrics\n",
    "\n",
    "\n",
    "In the next section, we are going to resample the \"normal\" class to match the number of samples within both \"normal\" and \"failure\" classes of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"15\"></a>\n",
    "## Dealing with Imbalanced Data\n",
    "\n",
    "When the data samples do not equally represent their classes, it is referred to as \"imbalanced data\" problem. In our example, we have __3196318__ samples of the normal hard drives, while this number for the \"failure\" class is only __234__. This clearly results in a model that is biased towards the \"normal\" samples, and the gradients of the \"failure\" class tend to vanish during the training epochs.\n",
    "\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      normal       1.00      1.00      1.00   3196318\n",
    "        fail       0.67      0.03      0.05       234\n",
    "```\n",
    "\n",
    "\n",
    "To mitigate the issue, in the next section we are going to resample the \"normal\" class to be roughly equal to the number of samples in the \"failure\" class (by a factor of 1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling training and test data sets for smaller, more balanced test\n",
    "num_normal_delta = round(train_fail_count * 0.2)\n",
    "num_normal_test_delta = round(test_fail_count * 0.2)\n",
    "\n",
    "# Sample training data\n",
    "df_tmp = df[df['failure'] > 0]\n",
    "sample_train_count_failed = df_tmp.shape[0]\n",
    "df_tmp = df_tmp.append(df[df['failure'] == 0].sample(n=(df_tmp.shape[0]+num_normal_delta)))\n",
    "sample_train_count_normal = df_tmp.shape[0]-sample_train_count_failed\n",
    "\n",
    "# separate training set features from labels\n",
    "df_train = df_tmp.drop(columns=['failure'])\n",
    "df_train_target = pd.DataFrame(df_tmp['failure'])\n",
    "\n",
    "# Sample test data\n",
    "df_tmp = df_t[df_t['failure'] > 0]\n",
    "sample_test_count_failed = df_tmp.shape[0]\n",
    "df_tmp = df_tmp.append(df_t[df_t['failure'] == 0].sample(n=(df_tmp.shape[0]+num_normal_test_delta)))\n",
    "sample_test_count_normal = df_tmp.shape[0]-sample_test_count_failed\n",
    "\n",
    "# separate test set features from labels\n",
    "df_test = df_tmp.drop(columns=['failure'])\n",
    "df_test_target = pd.DataFrame(df_tmp['failure'])\n",
    "\n",
    "# Output name data set sizes\n",
    "print('Sampled Dataset Sizes:')\n",
    "print('- Train: Number of Normal Disks :',sample_train_count_normal)\n",
    "print('- Train: Number of Failed Disks :',sample_train_count_failed)\n",
    "print('- Test:  Number of Normal Disks :',sample_test_count_normal)\n",
    "print('- Test:  Number of Failed Disks :',sample_test_count_failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"e2\"></a>\n",
    "### Exercise 2: Implications of balancing data samples\n",
    "\n",
    "Before proceeding with training, take some time and think about the implications of rebalancing the data. we have down-sampled the \"normal\" data points from 3M to ~320, a 10-thousand-time reduction in the number of samples. \n",
    "* How do you expect this down-sampling to affect each class? \n",
    "* What would be the effect on the precision/recall and f1-score values for each class? \n",
    "\n",
    "Write your answers below before training the model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "#Your answer to the above question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to build the cuDF data from the pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sampled data into cuDF Dataframe\n",
    "\n",
    "gdf_train = cudf.DataFrame.from_pandas(df_train)\n",
    "gdf_train_target = cudf.DataFrame.from_pandas(df_train_target)\n",
    "\n",
    "gdf_eval = cudf.DataFrame.from_pandas(df_test)\n",
    "gdf_eval_target = cudf.DataFrame.from_pandas(df_test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally perform training on the resampled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU, with cuDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('Converting cuDF data sets into XGBoost DMatrix format...')\n",
    "xgtrain = xgb.DMatrix(gdf_train, gdf_train_target)\n",
    "xgeval = xgb.DMatrix(gdf_eval, gdf_eval_target)\n",
    "print('Completed converting data sets')\n",
    "\n",
    "params = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "print('Starting GPU XGBoost Training with cuDF Dataframes...')\n",
    "bst = xgb.train(params, xgtrain, 20, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_gpu = time.time() - start_time\n",
    "print('GPU XGBoost Training with cuDF Dataframes compeleted - elapsed time:', timetaken_gpu,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's run it over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting XGBoost prediction on test dataset...')\n",
    "\n",
    "# Use trained model to predict on test data set\n",
    "preds = bst.predict(xgeval)\n",
    "\n",
    "# Convert predictions to \"normal\" (0) or \"failed\"\n",
    "y_pred = []\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "for pred in preds:\n",
    "    if pred<=THRESHOLD:\n",
    "        y_pred.append(0)\n",
    "    if pred>THRESHOLD:\n",
    "        y_pred.append(1)\n",
    "\n",
    "# Output array of classifications                \n",
    "y_pred = np.asarray(y_pred)\n",
    "        \n",
    "y_true = df_test_target.values.reshape(len(preds))\n",
    "print('XGBoost prediction completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally review accuracy with the new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (Eval)\", round(accuracy_score(y_true, y_pred),5))\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"normal\", \"fail\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the recall/precision values for the \"fail\" class are hugely improved. The cost we pay here is the reduction in the precision/recall values of the \"normal\" class. Could you explain some scenarios where each of these strategies would be applicable? \n",
    "Finally, Compare the obtained results with your answers to exercise 2. How closely were you able to predict the results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we learned how to work with a sample dataset and curate, normalize, and filter data to prepare it for our XGBoost model. We reviewed the XGBoost model and learned how to utilize the cuDF data structure to accelerate the training/testing phases for the XGBoost model. Moreover, we learned how to modify the data to mitigate the issue of imbalanced classes.\n",
    "In the next two labs, we will use deep-learning-based approaches for classification and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"All-answers\"></a>\n",
    "## Answers to selected exercises:\n",
    "---\n",
    "\n",
    "<a name=\"a1\"></a>\n",
    "**Exercise 1: More insights into data:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, (df != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](#e1) to go back"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
