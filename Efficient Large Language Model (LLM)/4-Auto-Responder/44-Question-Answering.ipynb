{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f7516-57f9-471f-8bd9-b080127a688b",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da992d2-2293-4519-8ebb-5a856db69642",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb453e-7613-4dc9-9715-087799586a53",
   "metadata": {},
   "source": [
    "In this notebook you will begin work on an extractive question answering task using the [Stanford Question Answering](https://rajpurkar.github.io/SQuAD-explorer/) (SQuAD) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f27af-228b-4e0e-b718-8345cb50f511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdf224-2849-43af-9265-e33d221b10b7",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e18c9-1f9e-4f67-8ad8-f5082c95b021",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Be familiar with the SQuAD question answering dataset.\n",
    "- Observe zero-shot performance for extractive question answering using GPT43B and GPT8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ec8-042a-456b-bdc9-457739a75344",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ce63-7179-405b-86cf-03e49f51f354",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807cfa5-a29a-4e8b-ac92-063655341429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1c11-09be-40f1-aceb-09535d298f0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c7df4-40ae-40dd-826c-93380de3c175",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb9dd1-f059-4397-b4c9-6b3a37263fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d25de-1012-4c43-9581-299bfb3aacd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017a817-edab-4029-abb1-4ed21b094ffb",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe245c-bdfc-4f32-9d32-1f4c6dd72f51",
   "metadata": {},
   "source": [
    "For the question answering task, we will be working with the Stanford Question Answering Dataset (SQuAD). From the SQuAD documentation:\n",
    "\n",
    "> SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "The dataset contains over 100,000 questions and either its answer, or, that the question in unanswerable from the provided textual context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc1065-234a-4478-9ae4-0ab82ff11058",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/squad.json', 'r') as f:\n",
    "    squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39352c75-eb4b-477e-81bd-1ab701cd1f0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eaa56-6967-41e2-949c-38b494372865",
   "metadata": {},
   "source": [
    "## Explore SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1424e-988e-4cc3-a1d2-79c1e8546b0c",
   "metadata": {},
   "source": [
    "The dataset comes as a dictionary with only 2 keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141479ac-bef7-4a0d-ad9e-b87914c36612",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdb4a6-9fe1-4986-b200-73c6b5ee8a26",
   "metadata": {},
   "source": [
    "We are entirely interested in `data` which contains 442 different topics, each with many textual contexts and then questions and answers based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e841a-1c54-4a30-8afd-d4ba096fc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = squad_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ad181-70b7-4084-8987-f64a5e895f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2c06c-d831-4484-af05-7c3c2c541519",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data[:10]:\n",
    "    print(f'Topic: {d['title']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb825456-c7e9-4b90-9b8d-23e049121245",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bdead-f9a3-4e95-b9d1-88899da0df2b",
   "metadata": {},
   "source": [
    "## Explore Beyoncé Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fc804-5cb8-4c8d-90ff-9ebcb9389a5a",
   "metadata": {},
   "source": [
    "Let's take a look at the first topic in the dataset, which is about the pop singer Beyoncé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8cafd-ee54-4b2a-99a3-643f8d2b2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "beyonce = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0955e-5d64-41d1-a11e-de7d1cb0ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "beyonce.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2677745-be2c-4fa0-a576-073bc58e0de4",
   "metadata": {},
   "source": [
    "Each topic contains a collection of context paragraphs that serve as the basis for the question answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208e554-14ef-473f-ad1a-64edd2c49ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = beyonce['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec38606-ab17-408e-8284-d88261e552e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950385dd-6f3b-4607-86bc-3291940335b9",
   "metadata": {},
   "source": [
    "In the case of the Beyoncé topic we can see that there are 66 context paragraphs, each with their own set of questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da968b69-3176-4408-b601-2cba66c94250",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb53932-30e1-4962-9869-a421ce1fc7a0",
   "metadata": {},
   "source": [
    "### Context, Questions and Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccb417-e07b-4320-8ce4-8b082f43f6fb",
   "metadata": {},
   "source": [
    "Let's look at the first contextual paragraph and its questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088814b-5243-4624-b171-838071641f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90dc91f-d9d8-4446-ac3f-7544bef91b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1b71c-2969-4a73-a829-de9fc775f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b37ec3-bc20-42e8-b7ee-0c74575ef477",
   "metadata": {},
   "source": [
    "This particular contextual paragraph has 15 question/answer pairs associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2966ab-90bf-4422-bb90-d47f7454c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qas = paragraph['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b092b-0259-4060-9a35-f1c8d47f32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d4784-1a44-43a5-ab16-91101ee7382b",
   "metadata": {},
   "source": [
    "Here's the structure of a single question/answer pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca62a2a-fbb3-4771-9a06-5d971dd14ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f14496-5eb1-4f49-95ad-7b9793434d32",
   "metadata": {},
   "source": [
    "Let's take a look at a few of the questions and their answers, and confirm that the answers are derived from text in the provided context paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243f862-d1e8-4e13-b761-5a2d372f9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in qas[:5]:\n",
    "    question = qa['question']\n",
    "    answer = qa['answers'][0]['text']\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Answer: {answer}')\n",
    "    print(f'Answer in paragraph: {answer in paragraph['context']}\\n') # See `paragraph['context']` above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa6a99-9d99-43f5-8917-22a7487af193",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e3a87-f628-4b1f-9a81-fdb33da4d91e",
   "metadata": {},
   "source": [
    "## Process SQuAD Data Into Context, Question, Answer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48882ad4-45bb-441c-93ba-0b080bab881e",
   "metadata": {},
   "source": [
    "Ultimately we are going to use SQuAD data to fine tune a model on a question answering task. To that end it will be helpful to process the SQuAD data to simplify its structure and create a list where each item contains a context, question, and answer.\n",
    "\n",
    "Knowing what we do about the structure of the SQuAD data above we can run the following cell to do just this.\n",
    "\n",
    "Note that SQuAD contains some questions that are intentionally impossible to answer based on the provided context. We are going to choose to ignore these questions and instead only use those that have a clear answer.\n",
    "\n",
    "Also remember that SQuAD contains over 100,000 questions and answers. We know that for PEFT we can typically do well with roughly 1000 samples. With that in mind, and to keep our dataset diverse, we are only going to take the first context paragraph and its questions and answers for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3297d-0ffe-456d-b0cb-77e5af0cc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_questions_answers = []\n",
    "for topic in data:\n",
    "    cqa = topic['paragraphs'][0]\n",
    "    context = cqa['context']\n",
    "    for qa in cqa['qas']:\n",
    "        if qa['is_impossible']:\n",
    "            continue\n",
    "        question = qa['question']\n",
    "        answer = qa['answers'][0]['text']\n",
    "        contexts_questions_answers.append({'context': context, 'question': question, 'answer': answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85becaa2-d6b4-4938-bc70-fe091238e629",
   "metadata": {},
   "source": [
    "This leaves us with over 2000 context, question, answer items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6d40b-e7a3-4a80-9403-2bbef9cb191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contexts_questions_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402db08-e107-4250-8799-f3a4997e3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_questions_answers[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b26799-3ee4-4df2-8495-0cbb430f5657",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439a5df-75a4-4d7e-a95a-0ad7661ba6be",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b623c-7a06-4c1a-b7e4-adf2c002c44c",
   "metadata": {},
   "source": [
    "Even though we only took the first context paragraph for each topic in the dataset, we still have many questions for each of those context paragraphs. With that in mind, let's shuffle the data.\n",
    "\n",
    "We set a random seed here for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e039c-339b-4980-9117-2edf4150c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0139f3-5f8e-4ee2-99ce-a6e56dcb9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(contexts_questions_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c32e4-8e19-4479-b054-dfd0e286e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cqa in contexts_questions_answers[:5]:\n",
    "    print(cqa['context']+'\\n')\n",
    "    print(cqa['question'])\n",
    "    print(cqa['answer']+'\\n-----\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50518039-cf91-49b9-901f-ca76b8cf5abe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcd383-3f3b-4ea2-b24f-fb0747d7def7",
   "metadata": {},
   "source": [
    "## Question Answering Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00284c-6e62-4c0a-a33b-e891252e95b7",
   "metadata": {},
   "source": [
    "We will continue the practice of denoting our LLM tasks with a prompt template function. In the case of extractive question answering, we will use the following, which constructs a prompt given a provided `text` context and the `question` we would like answered from the provided `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1531cac-4bc6-447e-821e-0709beda44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_template(text, question):\n",
    "    return f'{text}\\n{question} answer: '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102c0fb-0738-4985-9234-3bf783819c78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b419d-fe31-4a87-bb86-4715a7b3936c",
   "metadata": {},
   "source": [
    "## Create Prompts with Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df4950-e47c-4ffd-81b2-9fd5464a8b77",
   "metadata": {},
   "source": [
    "Now we can combine our `contexts_questions_answers` with the `extract_template` to create a list of prompts and their labels, which we will be able to leverage when working with our LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f343c-4cd0-47da-8bc6-2fd20360f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_answers = []\n",
    "for cqa in contexts_questions_answers:\n",
    "    context, question, answer = cqa['context'], cqa['question'], cqa['answer']\n",
    "    prompt = extract_template(context, question)\n",
    "    prompts_and_answers.append((prompt, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06adae-e23b-4959-a997-a5882a4811d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b0966-96dc-482d-9db2-041c81401f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[0:3]:\n",
    "    print(prompt+'\\n')\n",
    "    print(answer+'\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea93c88-30ef-41e3-823a-edb00cfa438e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3504da-258e-4964-942b-63531ef537b7",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting with GPT43B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6a9bf-2021-4233-83cd-fac70cf52ba3",
   "metadata": {},
   "source": [
    "Let's see how GPT43B performs on this extractive question answering task with straightforward zero-shot prompting. First we'll instantiate an instance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13379b59-6e19-43d3-bf25-86fefeb88652",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b = NemoServiceBaseModel(Models.gpt43b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11510a1a-ee81-44c5-a3bc-1f2e7e0f7045",
   "metadata": {},
   "source": [
    "Next we'll try it out on the first several prompts in `prompts_and_answers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d88ad-a1ce-4d60-b6a1-da4d7309f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[:5]:\n",
    "    response = gpt43b.generate(prompt).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Answer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58588073-ddc2-4ec2-8760-038baf1a6810",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e891a5b-44b1-482e-aba1-c865328959bf",
   "metadata": {},
   "source": [
    "At a glance, it looks like GPT43B is well suited for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c06c17-3464-4c51-90a4-4aa853dbfa37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3946a9-89eb-4533-bd95-0b1430cc22ab",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting with GPT8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde9aa9-2066-4720-b0f9-07b4f5214df5",
   "metadata": {},
   "source": [
    "Now let's see how the much smaller GPT8B does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336f0f4-92dd-428d-8f0b-a9c199a29e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt8b = NemoServiceBaseModel(Models.gpt8b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ef396-2c39-49f2-9081-c2d937875ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[:5]:\n",
    "    response = gpt8b.generate(prompt).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Answer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba3f08-484b-4852-82b0-cf58eb827d17",
   "metadata": {},
   "source": [
    "At the least, GPT8B seems to be going on and on, let's try again, indicating that we would like the model to stop generating after newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf13ff-b836-4b16-a694-e32a677f464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[:5]:\n",
    "    response = gpt8b.generate(prompt, stop=['\\n']).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Answer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b454a1-8f8d-43d5-b5e2-6471d8579b3c",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a209e6-78fc-40e2-9eb0-a20fb81c6988",
   "metadata": {},
   "source": [
    "GPT8B continues to generate much more than we would like. It often repeats itself. It does not appear to be providing an answer extracted from the provided context. It is sometimes (see \"Boris Yeltsin\") wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0ebb5-a999-4269-968a-74e7a0120df8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1ceba-a447-4741-97ef-53230d7ff2f6",
   "metadata": {},
   "source": [
    "## Write Prompts and Answers to File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42581925-87b0-472c-9656-5c27ad306b33",
   "metadata": {},
   "source": [
    "In the next section we will turn our attention to fine-tuning GPT8B on this task and it will be helpful to reuse the `prompts_and_answers` list that we created here. Let's write it to file so we can easily load it into the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7c771-c24d-4004-9b9b-3fe9a985c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/squad_prompts_and_answers.json', 'w') as f:\n",
    "    json.dump(prompts_and_answers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
