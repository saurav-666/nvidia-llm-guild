{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932e3a28-f5b5-497d-be11-3f29417a01a2",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0127a4-11ad-4da8-98ad-169eb1fdd79e",
   "metadata": {},
   "source": [
    "# Responding in Your Own Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6b9a3-fa39-4396-9de2-31aea17cc00b",
   "metadata": {},
   "source": [
    "In this final notebook you will refactor your `respond_to_email` function to automatically reply to customer emails in a distinct voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3d3c6-e597-402d-9de9-8f03a9436b89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3750eac-1586-4595-88a2-71cda688a162",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542133cf-b013-4dcd-835a-a57a0694e9a1",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Learn about a tool to help you train models to respond in your own distinct voice\n",
    "- Be able to modify LLM responses to be in a distinct voice\n",
    "- Reflect on everything you've learned over the course of this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dec35-d94d-441a-beb2-28b08fb9719d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5409d57-d29b-47a4-ad64-9233d17760a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.helpers import collect_my_prompts_and_responses\n",
    "from llm_utils.models import LoraModels\n",
    "from llm_utils.postprocessors import strip\n",
    "from llm_utils.llm_functions import (\n",
    "    make_llm_function,\n",
    "    get_sentiment,\n",
    "    extract_name,\n",
    "    extract_product,\n",
    "    extract_location,\n",
    "    generate_customer_response_email\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e3194-44c2-4e32-9378-ea7551f1168a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e327e-b625-4ca3-9b24-c505db87a79e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c555d33-be7c-449f-acf4-c5bbcdf661cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4d509-006c-4e91-a5c2-07ad6b2adb36",
   "metadata": {},
   "source": [
    "## Load Customer Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6d425-e35e-4dbe-af83-69d806d047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/solution_emails.json', 'r') as f:\n",
    "    customer_emails = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e916b-b9e4-402b-8c65-8f4cefd9c109",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433d4f6-e21d-4f96-9942-4db29a6b2f63",
   "metadata": {},
   "source": [
    "# Responding in Your Own Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e292f6e-e053-4041-a419-6e4492e77a18",
   "metadata": {},
   "source": [
    "As a very last step in our task to autogenerate customer emails, we would like our email responses to capture our own voice and style of writing. A fantastic way to accomplish this is to fine-tune a model on text data that you have have written yourself in a natural setting.\n",
    "\n",
    "As we've learned, fine-tuning can work quite well with ~1000 samples, and sometimes less. With this in mind you can either sit down and write responses to several hundred to a thousand \"prompts\", which you could curate and/or synthetically generate yourself, or, you could make it a point as you go about your day having hundreds of natural text interactions with others to collect \"your prompts and responses\".\n",
    "\n",
    "The following opens a simple web page where you can submit \"your prompts and responses\" and saves them to `my_prompts_and_responses.jsonl` formatted correctly for NeMo Service p-tuning or LoRA.\n",
    "\n",
    "As always, be responsible and take care not to send data to 3rd party services when you should not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df70e2-62b2-46ee-a8a1-a10d67183cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_my_prompts_and_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478062b-0232-4388-a283-43635523b385",
   "metadata": {},
   "source": [
    "You could even easily imagine adding some additional logic that uploaded the data and kicked off a fine-tuning job once your data got to a size you were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ab6ec-c70b-445f-abdd-04871b3af6bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016a6e3-fe10-4604-be4e-244759887fda",
   "metadata": {},
   "source": [
    "## Responding With a Pirate's Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ec611-0e23-4120-8a1a-864e53de9d1c",
   "metadata": {},
   "source": [
    "But since we don't have a bunch of data of your voice, and since if we provided you data of how we sounded you might not really be able to tell the difference, we've provided you with a model that has been LoRA fine-tuned on pirate responses. We call this, our final PEFT technique of the workshop, **P**irate-**tuning**. üè¥‚Äç‚ò†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995ad4-94af-4586-825b-ae3a2db976ef",
   "metadata": {},
   "source": [
    "![Persona Create](images/persona_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f80a88-ad4c-46b0-a06a-f5e05910109d",
   "metadata": {},
   "source": [
    "Continuing to leverage the virtuous cycle we've been discussing throughout the day, we generated our synthetic pirate response data by 2-shot prompting GPT43B to repeat the emails generated in the previous notebook (though we generated 700 instead of 50, since we had the time) in the voice of a pirate.\n",
    "\n",
    "We passed the original (synthetically generated) emails into the following prompt format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827f115-c505-4474-903f-baecff2d63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pirate_template(statement):\n",
    "    return f'Statement: {statement}Pirate Statement: '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f54b08-86c0-4e63-ba3e-57a2eb2a573b",
   "metadata": {},
   "source": [
    "...then saved GPT43B's responses as labels. We used the email/pirate-label pairs to perform LoRA for 1 epoch using GPT8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa75c75-54d9-4f9d-a988-d9a8b97a677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_model = NemoServiceBaseModel(LoraModels.gpt8b.value, customization_id='2d03bab3-0d79-44ff-b21f-bb6cea86b4fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d62e93-186c-4e05-9747-4a91cbea8553",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44db00-a9c6-426b-a2c2-ebe5c61cd9b0",
   "metadata": {},
   "source": [
    "## Pirate LLM Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c7068-75a8-43fc-a46c-3ba6a17c4399",
   "metadata": {},
   "source": [
    "![Persona LLM Function](images/persona_llm_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a71155-833d-4cfe-9c08-5f084edbe767",
   "metadata": {},
   "source": [
    "Here we build a `pirate` LLM function using the LoRA fine-tuned GPT8B model and the `pirate_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec289b-ae3f-4598-b698-5806edb497b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate = make_llm_function(pirate_model, pirate_template, postprocessor=strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212d63e-d056-4ccf-b674-650273a81fbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca039da3-067e-4437-b86a-fd5c3264eeaf",
   "metadata": {},
   "source": [
    "## Try Pirate Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbcdef-2bb7-44a6-a1e8-3bab875526b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"All of the sudden I know a lot about how to customize and orchestrate customized LLMs for fun and profit. \\\n",
    "Thank you prompt engineering. Thank you LoRA. Thank you NeMo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d5ad5-e839-4407-807d-c5247252d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate(prompt, stop=['\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5cf40-d9d3-482e-9e88-555616840c7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e5688-96e3-495d-8ec4-5be90caa849a",
   "metadata": {},
   "source": [
    "## Auto Generate Response Emails in \"Your Own\" Voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7c512-7110-4739-a077-e0f9fb0edb3f",
   "metadata": {},
   "source": [
    "Refactor `respond_to_email` to include its final step, making the response sound more just like \"you\" (assuming you sound like a pirate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e491521-73d6-4c2f-b5d0-066b583e6600",
   "metadata": {},
   "source": [
    "If you'd like to see a solution for this section, expand the _Solution_ section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643ffb7-7b36-40c9-920c-d3508db6a0f7",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7c04f-4f2f-4bca-8ead-14ca9bf23eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_email(email):\n",
    "    name = name_extractor(email)\n",
    "    sentiment = get_sentiment(email, tokens_to_generate=1)\n",
    "    product = product_extractor(email)\n",
    "    location = location_extractor(email)\n",
    "\n",
    "    response = write_response_email(company_name, name, sentiment, product, location)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf12143-8786-413b-98af-aa442ddc0c9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dfcde-bd81-4fdb-b6e1-6ef8e2c490e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_email(email):\n",
    "    name = extract_name(email)\n",
    "    sentiment = get_sentiment(email, tokens_to_generate=1)\n",
    "    product = extract_product(email)\n",
    "    location = extract_location(email)\n",
    "\n",
    "    company_name = 'StarBikes'\n",
    "    response = generate_customer_response_email(company_name, name, sentiment, product, location)\n",
    "    return pirate(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd714a2-c924-4b1d-97cc-eee2b25e08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer_email in customer_emails[:1]:\n",
    "    print(customer_email+'\\n\\n')\n",
    "    print(respond_to_email(customer_email)+'\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd8bb3-38ff-4999-87ba-2ded8c68a853",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a537b2c-0d03-4b28-bea6-58e46cd4e3b2",
   "metadata": {},
   "source": [
    "## Revisit Email Responder Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e152b-55dc-474a-9957-59f7c6209430",
   "metadata": {},
   "source": [
    "You may not have believed it when we first set out today, but by now everything in the slide deck we went through at the beginning of the course should make quite good sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d04900-4fe1-4879-858d-1e11900af7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.slides import load_respond_to_email_slides\n",
    "load_respond_to_email_slides()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
