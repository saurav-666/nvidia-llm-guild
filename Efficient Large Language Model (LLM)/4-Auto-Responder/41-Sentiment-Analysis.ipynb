{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f7516-57f9-471f-8bd9-b080127a688b",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da992d2-2293-4519-8ebb-5a856db69642",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb453e-7613-4dc9-9715-087799586a53",
   "metadata": {},
   "source": [
    "In this notebook you will begin work on a sentiment analysis task using a dataset of Amazon reviews by performing a baseline zero-shot analysis on 2 GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f27af-228b-4e0e-b718-8345cb50f511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdf224-2849-43af-9265-e33d221b10b7",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e18c9-1f9e-4f67-8ad8-f5082c95b021",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Be familiar with the Amazon reviews dataset.\n",
    "- Observe zero-shot performance for sentiment analysis on the reviews using GPT43B and GPT8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ec8-042a-456b-bdc9-457739a75344",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ce63-7179-405b-86cf-03e49f51f354",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807cfa5-a29a-4e8b-ac92-063655341429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1c11-09be-40f1-aceb-09535d298f0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c7df4-40ae-40dd-826c-93380de3c175",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb9dd1-f059-4397-b4c9-6b3a37263fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d25de-1012-4c43-9581-299bfb3aacd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017a817-edab-4029-abb1-4ed21b094ffb",
   "metadata": {},
   "source": [
    "## Amazon Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe245c-bdfc-4f32-9d32-1f4c6dd72f51",
   "metadata": {},
   "source": [
    "For the sentiment analysis task, we will be working with a public dataset of Amazon customer reviews. The raw reviews file has been provided for you at `data/reviews.txt`. It contains 400,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3bd8c8-97ae-41c4-ba8f-201230d63ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/reviews.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e875499-21a0-4b86-9dab-b591bf78bb80",
   "metadata": {},
   "source": [
    "If we look at the first few samples, we can see that each begins with either `__label__2` which indicates a positive sentiment, or `__label__1` which indicates a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205db7b-3de0-4a3f-b883-f29367eeb87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/reviews.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa6a99-9d99-43f5-8917-22a7487af193",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba284d1-47e9-444c-943e-d0a1fa7bdfb2",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0aa57-6039-498a-83f4-53c926ec61d9",
   "metadata": {},
   "source": [
    "For our sentiment analysis task, we will be working with the following prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eda9dc-abfc-4dd7-99eb-b47274580bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_template(text):\n",
    "    return f'Is the overall sentiment of the following review \"positive\" or \"negative\"? {review} Sentiment:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90124566-4319-457b-9db6-96f2928fd5e7",
   "metadata": {},
   "source": [
    "Assuming we have a review to pass into the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62eeab9-643e-48ed-b49a-60db83da3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = f'''\\\n",
    "One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I \\\n",
    "have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger \\\n",
    "which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. \\\n",
    "There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially \\\n",
    "like, as there's not too many of those kinds of songs in my other video game soundtracks. \\\n",
    "I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.\\\n",
    "My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, \\\n",
    "which I find distracting. But even if those weren't included I would still consider the collection worth it.\\\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101be73-a2b9-4f58-94ea-c496de9da4d8",
   "metadata": {},
   "source": [
    "...we can generate a sentiment analysis prompt for the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7823a8-298a-4eea-9e1f-3f082d320430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_template(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d9fdc-ade4-4730-b3e5-4040b99738f2",
   "metadata": {},
   "source": [
    "## Process Prompts and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a5f1a-362b-4edb-ad46-ea79693be749",
   "metadata": {},
   "source": [
    "For our purposes we will create a training dataset of 1500 samples, as well as a small test dataset of 20 samples.\n",
    "\n",
    "Here we gather the first 1520 samples into a `prompts_with_labels` list which contains 2-tuples of review prompts, created using `sentiment_template`, and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392486e-7124-4152-8c56-3208a9a5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_with_labels = []\n",
    "\n",
    "with open('data/reviews.txt', 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= 1520:  # Stop after reading 1520 lines\n",
    "            break\n",
    "\n",
    "        label, review = line.strip().split(' ', 1)\n",
    "        sentiment = 'positive' if label == '__label__2' else 'negative'\n",
    "        prompts_with_labels.append((sentiment_template(review), sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d23612-f333-481f-ac34-807cab44739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts_with_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8ec64-a42b-43f1-a2ad-57fc0d077379",
   "metadata": {},
   "source": [
    "Next we split the list into separate train and test lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22437ed-8124-4e12-94b8-619111b583b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts_with_labels = prompts_with_labels[:1500]\n",
    "test_prompts_with_labels = prompts_with_labels[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dad1e-495f-46c2-a2b2-4c76f4957f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_prompts_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b59c07-11f2-4c75-a318-6250ba8d15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_prompts_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5828c-dd80-4672-94d8-f0979a4bb983",
   "metadata": {},
   "source": [
    "## Write Data to File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1cbc8-a30c-4e75-a3ef-d1d7fb9f661b",
   "metadata": {},
   "source": [
    "For use in subsequent notebooks, we will now write the train and test prompts and labels data to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a845d15-607a-49df-9180-f9484b45abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sentiment_prompts_labels_train_1500.json', 'w') as f:\n",
    "    json.dump(train_prompts_with_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecc43e-412b-4b22-a8c0-2e0f3e9b09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sentiment_prompts_labels_test_20.json', 'w') as f:\n",
    "    json.dump(test_prompts_with_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14beda75-5bf4-45dc-9904-f2b21c92b690",
   "metadata": {},
   "source": [
    "## Test Models on Zero-shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e6a3b-9cd4-4896-acc6-0caa7cfbce88",
   "metadata": {},
   "source": [
    "Before we begin work on fine-tuning, let's establish a baseline for performance by using our zero-shot prompts with GPT43B and GPT8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259425d7-ee75-4d85-9f43-850dd84e1a40",
   "metadata": {},
   "source": [
    "## GPT43B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d5669-105a-4c07-922f-fb028c70a787",
   "metadata": {},
   "source": [
    "First we create an instance of the GPT43B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa299557-f32e-4d5c-9d22-49d0fc7c896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b = NemoServiceBaseModel(Models.gpt43b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc279d04-e7b9-4279-8bba-7283faef5936",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837665ae-1012-462d-8b97-27e905a88db1",
   "metadata": {},
   "source": [
    "Let's try a single sentiment analysis prompt out on GPT43B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa289e6e-4cb6-4268-b6a4-26901cdd53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, label = test_prompts_with_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dab0e-4a30-409d-9b97-bd38cf33ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37419306-5b5c-409c-a24f-22aac4a99965",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c04f4-1491-4854-b6e1-79131402cea8",
   "metadata": {},
   "source": [
    "Except for some white space we can strip, it looks pretty good so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1ba6c-6d0e-41ef-b5d9-ca9f65481f21",
   "metadata": {},
   "source": [
    "### Try on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170e08f-e36a-4c56-91bc-cd6b24f3ac2a",
   "metadata": {},
   "source": [
    "Let's try GPT43B on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108dbd4-bbf8-42d6-8c0b-4641b578f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_prompts_with_labels)\n",
    "for prompt, label in test_prompts_with_labels:\n",
    "    response = gpt43b.generate(prompt).strip()\n",
    "    is_correct = response == label\n",
    "    if is_correct:\n",
    "        num_correct += 1\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Label: {label}')\n",
    "    print(f'Is Correct: {response == label}\\n')\n",
    "\n",
    "print(f'Number Correct: {num_correct}/{num_samples}')\n",
    "print(f'Percentage Correct: {num_correct / num_samples*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd323ff8-5600-454f-9308-bc3e0a58e1fd",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226deb5-fa8a-4147-a879-faffb5340ded",
   "metadata": {},
   "source": [
    "GPT43B seems to be well-suited out of the box for this sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abbae1-e8ac-45be-b600-2cb4efd0a60d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87dc008-a1a5-4a15-8c99-1f76179b917e",
   "metadata": {},
   "source": [
    "## GPT8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2d97d-f5fb-4423-8d0f-61ac52b49ca9",
   "metadata": {},
   "source": [
    "Next we will try with GPT8B. First we create a model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc71ef-0683-4a9a-b864-104ebad1825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt8b = NemoServiceBaseModel(Models.gpt8b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47191e65-2169-466f-9ba3-787750d965af",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f614013-5a72-4a56-80b3-8820bc3c26f3",
   "metadata": {},
   "source": [
    "Let's try a single sentiment analysis prompt out on GPT8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9bd8f-afc0-4c02-93c0-686af13819d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, label = test_prompts_with_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb737f7-7d97-40e1-8785-1722b86ef9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7e912-ad5a-4abc-b53a-a0f1c100132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt8b.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8168f33-2e2b-4775-9ea3-d4f22d30f693",
   "metadata": {},
   "source": [
    "GPT8B gave us a the correct sentiment, but then went on long after we wished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60752073-3065-462e-815f-2011c973bbf5",
   "metadata": {},
   "source": [
    "### Try on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d00b72-6935-4045-bc2e-9b08b1ba72ae",
   "metadata": {},
   "source": [
    "Let's try GPT8B on the full test set. We will indicate that we wish the model to stop generating after newline characters, strip white space, and lower case its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c346b0-bead-43f8-81f4-3b1412e6736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_prompts_with_labels)\n",
    "for prompt, label in test_prompts_with_labels:\n",
    "    response = gpt8b.generate(prompt, stop=['\\n']).strip().lower()\n",
    "    is_correct = response == label\n",
    "    if is_correct:\n",
    "        num_correct += 1\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Label: {label}')\n",
    "    print(f'Is Correct: {response == label}\\n')\n",
    "\n",
    "print(f'Number Correct: {num_correct}/{num_samples}')\n",
    "print(f'Percentage Correct: {num_correct / num_samples*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b098c4-d7d9-4f0b-bda8-2fbfe36c8d36",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f725da-371c-4d70-9aeb-6aa9fa8f13fa",
   "metadata": {},
   "source": [
    "GPT8B did pretty well on this task, although we had to rely on a fair amount of post-processing, including a stop character to prevent it from going on long after we wished.\n",
    "\n",
    "Looking at the outputs above, it missed at least a couple on account of including a period at the end of its output, and we see that it still got the wrong sentiment on occasion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
