{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e37aa61-051a-4fd0-aaae-85d30b6d1edd",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29a21f-f039-40b3-aca6-ad982d20393c",
   "metadata": {},
   "source": [
    "# Apply LoRA to the List Generation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c71f0c-8195-444c-b9e9-51592939e04e",
   "metadata": {},
   "source": [
    "Now that you've prepared prompt and label data for list generation, it's time to do parameter efficient fine-tuning on a GPT8B model for list generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c794ef-4403-4548-8844-4a837702b326",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3524dee-252a-4de5-b36d-ee3444501def",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2c380-6bbf-4f4b-80a6-f9bdf96325c3",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- LoRA fine-tune a GPT8B model for list generation.\n",
    "- Evaluate the fine-tuned models performance and articulate its limitations.\n",
    "- Create a list generation LLM function for easy reuse of the custom model's intented functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573dca7b-37ce-4b65-ab29-d7923af3be3a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facce48-eabe-4e8d-a574-571fecfd858c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd69037-5e1b-48be-baf5-69505f45e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import LoraModels\n",
    "from llm_utils.mocks import create_list_gen_lora_customization as create_customization\n",
    "from llm_utils.helpers import plot_experiment_results\n",
    "from llm_utils.prompt_templates import gen_list_template_zero_shot\n",
    "from llm_utils.llm_functions import make_llm_function\n",
    "from llm_utils.postprocessors import postprocess_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b359-6810-4c3d-82be-91e5c81d7029",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675681e8-f9e1-4128-8b81-b0249f1f4356",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf7cfe-331a-4baa-9168-637bd9c9f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b8b67-75dc-41bd-b587-a1f3333ce3fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8a8bb-cc9d-4137-9dce-22c554d8844c",
   "metadata": {},
   "source": [
    "## Training and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f62d79-4ca7-4af9-9c25-bc9d461f1e37",
   "metadata": {},
   "source": [
    "In the last notebook you generated 100 prompt/response training samples for list generation. Using the same process we generated about 900 samples, and split them into train, validation, and test data. In the case of the train and validation data we also formatted the data appropriately for NeMo Service customization and uploaded them to the NeMo Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f9f67-cacb-4823-b536-3a247aff70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/list_gen*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafc3a5-00c3-441b-9b95-dd324760dd67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0128bac-529b-49ba-8671-32cb3173cee1",
   "metadata": {},
   "source": [
    "## Dataset File IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5c105-cc4c-4063-bf52-f8aad6c1f935",
   "metadata": {},
   "source": [
    "After uploading the training and validation data, we obtained the following file IDs, which you will need to use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f2831-7f59-4bd4-b7bd-f66382ecd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_file_id = '85218a48-86a5-46d8-94cf-96a24f3078fa'\n",
    "validation_dataset_file_id = '419c55e3-2fbc-41cb-9bed-c0482f3ba26d'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a831b-a9aa-4e31-86b5-180a6ef1cc30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84071be8-9a26-41e1-8236-efb30c2ff5f2",
   "metadata": {},
   "source": [
    "## Exercise: LoRA Fine-tune GPT8B for List Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e68cda-f871-4a73-89eb-31f6a7f72858",
   "metadata": {},
   "source": [
    "![List Gen LoRA](images/list_gen_lora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c6970-41a5-414a-bdc1-6825f8604992",
   "metadata": {},
   "source": [
    "Your task is to launch a LoRA customization job for list generation using the appropriate GPT8B model. Instead of establishing an actual connection to NeMo Service with `conn.create_customization`, you will use our mock `create_customization` function imported above. If you don't pass in the correct arguments, the function will tell you. When you do successfully provide the correct arguments for the LoRA customization, the function will provide you with a customization ID for a LoRA customization we have already completed, that you will us later in this notebook.\n",
    "\n",
    "We ran our training for 50 epochs, but the training exited after 13 epochs, having gone for 10 epochs without any improvement to the validation loss. The best epoch was 3 and the checkpoints for this epoch are what are used in the customization. Therefore, for this customization, train for 3 epochs.\n",
    "\n",
    "For your reference, here is the solution from the previous PubMedQA LoRA customization job you performed earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428507b-5989-487d-953a-f66685afc7c7",
   "metadata": {},
   "source": [
    "```python\n",
    "create_customization(model='gpt-8b-000-lora',\n",
    "                     training_dataset_file_id='cb1aab08-e396-41a8-9334-571c6672033d',\n",
    "                     validation_dataset_file_id='42d75e3a-7aa9-46fa-b1c0-63d7a66f7a8f',\n",
    "                     adapter_dim=32,\n",
    "                     epochs=3)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a538a-2bd2-4b28-8efb-1124b7035626",
   "metadata": {},
   "source": [
    "If you get stuck, feel free to check out the solution below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aef87f-87e8-4062-8693-c9f722e15720",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd7926-7ae5-4851-88b3-f0517cf94b5b",
   "metadata": {},
   "source": [
    "Correctly launch a (mock) LoRA customization using `create_customization` immediately below. On success, when you ascertain the customization ID, set the `customization_id` variable below to it for use later in the notebook.\n",
    "\n",
    "If you get stuck, feel free to check out our *Solution* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fc521-1bc5-4c69-a356-cb7774853683",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_customization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813c71b-9b1a-43dd-92ef-40b1103b9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "customization_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ab2f4-4ccf-492a-a2d0-ae79793e261c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9a713-6b6e-4a24-8d62-a46fb6097271",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_customization(model='gpt-8b-000-lora',\n",
    "                     training_dataset_file_id=training_dataset_file_id,\n",
    "                     validation_dataset_file_id=validation_dataset_file_id,\n",
    "                     adapter_dim=32,\n",
    "                     epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ff0d0-e6a6-4d65-8c77-44c74550f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "customization_id = '03f25d3b-715d-44cb-b682-61ef6f7df476'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819dfd02-a2b6-42a2-b2e4-18691253675b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c25f6d-a021-4b98-afb5-ec8fe2c53dc0",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0db7ef-a62d-4d1a-b98e-bcfb426598b9",
   "metadata": {},
   "source": [
    "Now you're ready to evaluate the performance of our LoRA fine-tuned GPT8B model on list generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f95b56-dc6b-48fe-b126-e81925e8cd5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3586d0d-fbfd-4989-a98e-2496e179a00b",
   "metadata": {},
   "source": [
    "## LoRA Customization Model Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f77c8e-c170-4ee7-a5f8-43f0ac70854f",
   "metadata": {},
   "source": [
    "Run the cell below to instantiate an instance of the LoRA fine-tuned GPT8B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e6da4-1971-4362-96c6-efd8ee71ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NemoServiceBaseModel(model=LoraModels.gpt8b.value, customization_id=customization_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983798a5-b938-4d6a-a9bb-1a77a15fc4f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255adc11-9a3a-4227-b521-f4bf8a07585b",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4de38c-fccf-49d6-b1c4-77ea6d959cd5",
   "metadata": {},
   "source": [
    "As mentioned earler, we've created test data for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604cc5c-5464-4b11-9199-8dd6b62abbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_labels = json.load(open('data/list_gen_test_25.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c45a0-f114-42d9-8428-aa15b9bf53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427dcfa-7608-4b2c-bd4b-55d31e9c76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in prompts_and_labels[:3]:\n",
    "    print(prompt)\n",
    "    print(label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8d280-5c1e-4135-8864-a572b0f93423",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749691ae-d596-4629-a19b-c06745a5faf6",
   "metadata": {},
   "source": [
    "## Try LoRA Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770f1b9-e024-4a43-8fe9-90ee69b38421",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in prompts_and_labels[:4]:\n",
    "    response = model.generate(prompt, return_type='text', stop=['\\n']).strip()\n",
    "    print(prompt)\n",
    "    print(response+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674abfa-3c46-40b2-a17a-ed1ceee6d570",
   "metadata": {},
   "source": [
    "So far this looks pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637090c-373e-4d53-a4bb-a9ae5ae332cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64933e4-929f-4eeb-9317-ff3bd08fdeb1",
   "metadata": {},
   "source": [
    "## Testing Generated List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b0ee4-c242-4092-8340-725a825544f0",
   "metadata": {},
   "source": [
    "We need to consider how we are going to evaluate the perfomance of our custom model given that we have no guarantees about the actual list items it is going to produce. For example, for the prompt \"Make a python list of 2 colors.\" both `['green', 'blue']` and `['silver', 'gold']` would be appropriate responses.\n",
    "\n",
    "In other scenarios we might very well wish to go through the effort of providing a way to evaluate that the items are appropriate to the `topic` that was passed into `generate_list`, perhaps using an LLM to help us. However, for the sake of time, and also because in all our experiments with list generation thus far we haven't seen much, if anything, in the way of off-topic responses, let's just agree that we need to test that we have a well-formed list, and that it needs to be of the appropriate length.\n",
    "\n",
    "To that end we will use the following function that will convert our model's generated list, confirm it is a well-formed list, remove any duplicate entries and return the list's length. If the model response is not a well-formed list, we will return `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81baebac-c1a8-47c7-86fb-1e48078ebbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_gen_list_len(response: str) -> str:\n",
    "    try:\n",
    "        literal_list = ast.literal_eval(response.strip())\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    try:\n",
    "        num_unique_items = len(set(literal_list))\n",
    "    except:\n",
    "        return -1\n",
    "    return num_unique_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3de44-de94-49d5-ac55-cd2f5bc04659",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_list = \"['a', 'b', 'c']\"\n",
    "bad_list_with_duplicates = \"['a', 'b', 'a']\"\n",
    "bad_list_formed_wrong = \"['a', 'b', 'c']]\"\n",
    "\n",
    "lists = [good_list, bad_list_with_duplicates, bad_list_formed_wrong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765a6f3-9024-4906-87d0-0938a9048010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lists:\n",
    "    print(get_good_gen_list_len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66e022-41b7-41c7-a8c5-725b06200645",
   "metadata": {},
   "source": [
    "In this approach, our labels should be the target length. Let's assume with the above toy lists we intended for them all to have a length of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4eaf1-7811-4780-848d-50c5270104b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lists:\n",
    "    test_result = get_good_gen_list_len(l) == 3\n",
    "    print(f'List {l} is correct: {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec91eb-b293-4a48-ab28-838c834647f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71b301-c23b-417b-928e-d9c4a3430067",
   "metadata": {},
   "source": [
    "## Try LoRA Fine-tuned Model on Full Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4f988-7d3e-4f8c-bdd7-36c672f7cb5b",
   "metadata": {},
   "source": [
    "Let's run through the test data using the technique we described just above. In the case that we get an incorrect response, we will print the prompt and response so we can investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92f028-07d6-491a-8e26-44a0e9b60385",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in tqdm(prompts_and_labels):\n",
    "    response = model.generate(prompt, return_type='text', stop=['\\n']).strip()\n",
    "    response_len = get_good_gen_list_len(response)\n",
    "    is_correct = response_len == len(label)\n",
    "    if not is_correct:\n",
    "        print(prompt)\n",
    "        print(response+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647dfcc-5d6c-4a46-a464-6e63ab4db7e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2ee79-473c-4ccf-b02d-86d316060fd1",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f65ebe-bbc0-4461-bc41-c2d6a686b796",
   "metadata": {},
   "source": [
    "For the most part it looks like our model had difficulty not introducing duplicates when the target length was 8 or 9. We could try to address this in the following ways:\n",
    "\n",
    "- Only use this model to generate lists of length 7 or less when possible duplication is an issue.\n",
    "- Try to adjust `top_k` and `temperature` to see if introducing randomness results in a reduction of inputs.\n",
    "- Iterate on our prompt template used in training:\n",
    "    - Instead of \"Make a python list of {n} {topic}\" try \"Make a python list of {n} {topic} without any duplicates.\"\n",
    " \n",
    "There was also one response, `['The Hitchhiker's Guide to the Galaxy']`, where the item in the list contained an apostrophe `'` that caused the list to be malformed. We might consider post-processing to handle this kind of output.\n",
    "\n",
    "Of course, we also might be perfectly satisfied with the list generator not always performing perfectly. A simple guard in our code like:\n",
    "\n",
    "```python\n",
    "response = generate_list(n, topic)\n",
    "if not response:\n",
    "    # Do something here in case we didn't get a well-formed response\n",
    "```\n",
    "\n",
    "...could be completely appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d8e48-b398-424f-92bb-ec6ae532f1f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859e38a-23d0-4836-b29f-de21712d3c75",
   "metadata": {},
   "source": [
    "## Compare LoRA Fine-tuned Performance With Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a510e-76e8-42ef-884e-68c3f20ce67d",
   "metadata": {},
   "source": [
    "We conducted evaluation with the same small test data set on a variety of models for zero, one, two and three shot learning, in addition to LoRA fine-tuned where possible. As we discussed in this notebook, we converted our generated list labels to their length and used `get_good_gen_list_len` to parse the output of our models' responses to compare to the length of the label.\n",
    "\n",
    "For your reference, for each model tested we did:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c13279-013d-443b-9df8-1f286827ce81",
   "metadata": {},
   "source": [
    "```python\n",
    "model.evaluate(prompts_and_label_lengths,\n",
    "                get_clean_prediction=get_good_gen_list_len,\n",
    "                stop=['\\n'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e3b4c-10b8-4786-bfec-31b85a334689",
   "metadata": {},
   "source": [
    "The results follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41658ed8-d4b8-4aaa-bc43-09f49e74f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_experiment_results('experiment_results/solutions/list_gen_experiment_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3911b4-707a-4f82-92b1-679509ca7eae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343f987-0f98-4f89-ae19-8f2d0cd3fc53",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1243c5-dfc8-46ea-9375-96e7628fad2e",
   "metadata": {},
   "source": [
    "LLaMA-2-70B and especially GPT43B performed the best at list generation, especially GPT43B which with few-shot learning was able to acheive perfect accuracy on our test set. Interestingly, and for reasons that would require additional investigation to understand, GPT43B LoRA did not perform as well as few-shot learning. Assuming accuracy, and especially with larger list lengths, were a priority at any cost, One-shot GPT43B would appear to be a reasonable choice.\n",
    "\n",
    "GPT8B improved its performance the more shots we gave it, but performed the best with LoRA with 76% accuracy. As discussed earlier in the notebook, GPT8B with LoRA did poorly with lists of length 8 and larger, producing well-formed outputs, but with repeat entries. Depending on the application, this may or may not be an issue and also, could be worked around by keeping list lengths to 7 or less. Assuming we would be willing to work with this constraint in some fashion, using GPT8B with LoRA gives us a fast response while using very few tokens and with a relatively small model.\n",
    "\n",
    "GPT20B performed slightly better than 8B. For this list generation task given that it is 2x GPT8B with LoRA and performed only slightly better, we would likely not choose it.\n",
    "\n",
    "LLaMA-2-70B-chat did a surprisingly poor job on this task, largely because of its tendency to be so \"helpful\" and provide extra conversational text. However, we note that with each shot its performance increased drastically, therefore, if for other reasons we wanted to work with this model, we might consider 4 or 5 shot prompting it to see how that impacted its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1485a84-f4f8-4630-8cb0-b3be65f421ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd40e2b-74f6-43eb-a852-61e4a462b8d1",
   "metadata": {},
   "source": [
    "## Exercise: Create LLM Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874b332-313d-49c8-b44e-3ad537b1d1c0",
   "metadata": {},
   "source": [
    "![List Gen](images/list_gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fa077-0de4-48ef-903d-9679d4d2975d",
   "metadata": {},
   "source": [
    "Now that we have a customized model and are satisfied with how it is working, let's take the time to create an LLM function that encapsulates the list generation task.\n",
    "\n",
    "As a reminder, we use `make_llm_function` to return an LLM function. `make_llm_function` expects the following arguments:\n",
    "1. A model instance: in this case use your LoRA-customized GPT8B model.\n",
    "2. A prompt template: in this case use `gen_list_template_zero_shot` provided below.\n",
    "3. An optional postprocessor: in this case use `postprocess_list` provided below, which will deduplicate your list and return an empty list in the case of model responses that are not well formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b53378-b140-48c7-ab87-1237a0352ab2",
   "metadata": {},
   "source": [
    "### List Generation Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a63809-f839-41ba-be77-28574c85adc1",
   "metadata": {},
   "source": [
    "It's worth noting that in our prompt engineered list generator we did few-shot learning, but with our fine-tuned model this is no longer required. Thus the following prompt template, intended for zero-shot prompts, will look different than our earlier few-shot prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18318a-6a8f-4401-bfa3-049d6bd3afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_list_template_zero_shot(n, topic):\n",
    "    return f'Make a python list of {n} {topic}.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3caa7-8a15-483d-ac44-836fa9e915ff",
   "metadata": {},
   "source": [
    "### List Postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7359d-4277-4f81-ae6a-5cf57f4b550f",
   "metadata": {},
   "source": [
    "The following, which you have seen before, will return an empty list in cases where the model's response is not a well-formed list, and will also de-duplicate any lists generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52177aec-a9b2-4ec3-bdf9-a212e84d2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_list(list_str):\n",
    "    try:\n",
    "        literal_list = ast.literal_eval(list_str)\n",
    "    except:\n",
    "        literal_list = []\n",
    "\n",
    "    return list(set(literal_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28949bf0-8131-475c-8c25-68df6b2cdc14",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef1ecb-d2bf-4063-9141-25659947d9d3",
   "metadata": {},
   "source": [
    "If you get stuck, feel free to check out the *Solution* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708ee37-b225-437e-a7bc-fa378eb79d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_list = 'TODO' # TODO: Make the `generate_list` LLM function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21cae6c-3184-4a55-838a-7fd6a5dbb840",
   "metadata": {},
   "source": [
    "### Test Your Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c34e0-68e2-4008-95c6-f71fd274f5db",
   "metadata": {},
   "source": [
    "Once you have implemented `generate_list` you should be able to run the following cell without any issues, and see your function generating well-formed lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23181d-8255-459f-a134-c5b2cff35c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [3, 4, 5, 6]\n",
    "topics = ['first names', 'cities in Maryland', 'bike products', 'moods']\n",
    "for n, topic in zip(ns, topics):\n",
    "    response = generate_list(n, topic)\n",
    "    print(f'Make a python list of {n} {topic}.')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafa739-56c2-4e48-98a9-773c5bc32351",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e622bdd-4456-4918-bfa8-f9fe5ed1f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_list = make_llm_function(\n",
    "    NemoServiceBaseModel(LoraModels.gpt8b.value, customization_id='03f25d3b-715d-44cb-b682-61ef6f7df476'), \n",
    "    gen_list_template_zero_shot, \n",
    "    postprocessor=postprocess_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a16d4a-b538-4d27-a010-474ff717d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [3, 4, 5, 6]\n",
    "topics = ['first names', 'cities in California', 'bike products', 'moods']\n",
    "for n, topic in zip(ns, topics):\n",
    "    response = generate_list(n, topic)\n",
    "    print(f'Make a python list of {n} {topic}.')\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
