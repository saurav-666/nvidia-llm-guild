{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f7516-57f9-471f-8bd9-b080127a688b",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da992d2-2293-4519-8ebb-5a856db69642",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLM Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb453e-7613-4dc9-9715-087799586a53",
   "metadata": {},
   "source": [
    "In this notebook we introduce the idea of LLM functions where we wrap task-specific LLM behavior into functions to promote modularity and code reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f27af-228b-4e0e-b718-8345cb50f511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdf224-2849-43af-9265-e33d221b10b7",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e18c9-1f9e-4f67-8ad8-f5082c95b021",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Wrap task-specific LLM functionality in modular, reusable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ec8-042a-456b-bdc9-457739a75344",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ce63-7179-405b-86cf-03e49f51f354",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807cfa5-a29a-4e8b-ac92-063655341429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "from llm_utils.models import Models, LoraModels\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1c11-09be-40f1-aceb-09535d298f0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c7df4-40ae-40dd-826c-93380de3c175",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb9dd1-f059-4397-b4c9-6b3a37263fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b12aad-778a-4d7c-988f-3c18cf45d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d25de-1012-4c43-9581-299bfb3aacd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7266747-bf45-46c4-90c1-137b9247e8c6",
   "metadata": {},
   "source": [
    "## Coding Best Practices in the Context of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04811d7b-6121-4652-8658-55bff0e9ea84",
   "metadata": {},
   "source": [
    "Whether we are working with LLMs or not, we should adhere to good general coding practices.\n",
    "\n",
    "Currently in this workshop our use of LLMs has primarily been for one-off tasks, but as we proceed through the workshop we will be wanting to compose the functionality of several LLMs that each perform specific tasks. To this end we should consider how we can organize the model functionality that we are and will be creating to facilitate our end goals.\n",
    "\n",
    "Our approach will be to create what we will call **LLM functions**. Each LLM function will ecapsulate everything necessary to perform a specific **task** using an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b9d81-99f8-404b-8b7e-4cbee934a16e",
   "metadata": {},
   "source": [
    "### Many Good Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854a40f-e5d9-464d-9dc0-427de4e0e2d6",
   "metadata": {},
   "source": [
    "We want to make clear that our approach here is by no means the only way to keep your LLM task functionality well organized. In your own codebases you will likely arrive at your own best practices. You may very well also be using 3rd party libraries such as [LangChain](https://www.langchain.com/) to help organize your LLM functionality.\n",
    "\n",
    "In any case, you are advised to think through how to make your organization's LLM functionality well organized and well suited for optimal use and/or reuse.\n",
    "\n",
    "In today's workshop we will adhere to a specific definition of an LLM function. In doing so we will better understand the role of model customization in the service of actual application functionality, and, will have a structure by which we can, as the workshop proceeds, accomplish many tasks both through the use of individual and composed LLM functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd90bdd-2118-4f4d-a321-2040b61d94c1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820a6e3-3170-4a20-aefd-bb0de5c5746f",
   "metadata": {},
   "source": [
    "## Components of an LLM Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f26ec-e3b4-4602-9318-d4e4f4f7b289",
   "metadata": {},
   "source": [
    "In our approach, an LLM function will comprise of 2 main components:\n",
    "1. An LLM instance\n",
    "2. A prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138ed17-12fc-4e16-895e-896709d659da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce98de-859e-40e7-a957-cfe681354be2",
   "metadata": {},
   "source": [
    "## LLM Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f647d-84a6-4975-8e22-c60c83d49d64",
   "metadata": {},
   "source": [
    "LLMs are the workhorses of our code bases today, so it goes without saying that an LLM function should leverage an LLM.\n",
    "\n",
    "In the following examples we will use NeMo GPT43B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5824bb-07df-46ee-ba81-2ff4e752f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NemoServiceBaseModel(Models.gpt43b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db40cd-a8e3-4ce6-9c7c-db2acdc8dc76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054bdd9-4907-425d-9802-bc45166c4974",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa911f-95e3-45e1-b93c-7111093ddfe4",
   "metadata": {},
   "source": [
    "The specifc task that an LLM is intended to perform is well articulated by a prompt template. For example, given the same LLM, we might wish to perform summarization, question answering, translation, or classification. For each of these examples we could denote the task with an appropriate task template. Here we provide examples for each, using functions that expect the necessary arguments to contruct an appropriate prompt utilizing the prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4c999-217e-4c38-931f-bfc8bf7433ee",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f120521-d050-4f59-b6f7-88fb4ef1f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_template(text):\n",
    "    return f'Summarize the following article:\\n{text}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0870cd-2e2b-45f8-ac7f-c26043c5f704",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865afd9-5f2e-490e-93ba-0bd440760674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_template(context, question):\n",
    "    return f'Given the context: {context}\\nAnswer the question: {question}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87559fba-474d-4327-a962-6918d9d97358",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55adeec-843c-4545-9a35-4a8c090ede3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_template(text, source_language, target_language):\n",
    "    return f'Translate the following text from {source_language} to {target_language}:\\n{text}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07873054-3706-4774-b51d-4aec8a9b74be",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78410b-fc3e-4147-83eb-8f9364ed6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_template(text):\n",
    "    return f'Classify the following text into \"Spam\" or \"Not Spam\":\\n{text}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a19076-129c-48fa-af4b-831a03d6db45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1ebff-aecd-441c-92da-714dd9539006",
   "metadata": {},
   "source": [
    "## Making LLM Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e20d9-17bb-4d36-bb40-765310954da0",
   "metadata": {},
   "source": [
    "Now that we have a model instance and several prompt templates, let's create a few naive LLM functions. Each function will leverage our model instance and one of the example prompt templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd3a30-73b9-42c3-94b9-57729338b7a6",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd9ece-0173-44a9-ae5d-b1910789c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    prompt = summarization_template(text)\n",
    "    return llm.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4fb80-baa8-400a-9ad4-03fed94733dd",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11190a4-aeff-4253-be12-a664112b721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(context, question):\n",
    "    prompt = qa_template(context, question)\n",
    "    return llm.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88f9e8-37fa-4f46-be06-0f8afb292185",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4f221-d9f6-4a09-9239-d497a54917be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, source_language, target_language):\n",
    "    prompt = translation_template(text, source_language, target_language)\n",
    "    return llm.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a42317-9da8-4e9a-ba74-af03bd04f15c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a0085-430a-44e0-813e-b8be4f6a41c3",
   "metadata": {},
   "source": [
    "## Try Naive LLM Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17aa74-7fce-48f3-aaae-85fee2aefed1",
   "metadata": {},
   "source": [
    "Let's try out our LLM functions on the following paragraph espousing the use of modular code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4239d2-7e82-4446-a10d-4498ff895a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "modular_code = f\"\"\"\\\n",
    "Single-purpose modular functions represent a cornerstone of efficient and maintainable software development, \\\n",
    "embodying the principle of doing one thing and doing it well. By encapsulating a specific \\\n",
    "task or calculation within a self-contained unit of code, these functions enhance readability, \\\n",
    "facilitate debugging, and promote reuse across different parts of a project or even among different projects. \\\n",
    "This modular approach allows developers to build complex systems through the composition of simpler, \\\n",
    "well-understood pieces, significantly reducing the cognitive load required to understand or modify the system. \\\n",
    "Moreover, single-purpose functions make unit testing more straightforward, enabling developers to \\\n",
    "verify the correctness of each part in isolation before integrating them into a larger system. \\\n",
    "Adhering to this best practice not only speeds up the development process by enabling code reuse and parallel \\\n",
    "development but also contributes to the creation of more reliable and easily adaptable software systems.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46ab63-0e1a-400c-8978-e3ab8e1a5949",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7d47b-60e5-4d16-a745-9dc16bdb7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(modular_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f0f68-3712-41c4-beb7-d8672afce22e",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82693cb5-4840-42f9-b3f8-5404448289c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(modular_code, 'What is modular code\\'s effect on debugging?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12cfc1-c67d-49c2-938d-007f067c8c41",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39265ed8-fa83-4036-b367-f3a36d158f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(modular_code, 'English', 'Mandarin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a15838-95bb-4834-9b3d-4d0ae3ae7494",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84279c19-a70a-4eed-8ba5-a6140fab0f16",
   "metadata": {},
   "source": [
    "## LLM Function Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6af2af-58c3-4db3-83fc-19429ff7af36",
   "metadata": {},
   "source": [
    "The above LLM functions worked great, but to make their creation even easier going forward, we will provide the following `make_llm_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555cad1-d3a3-448a-8ec0-7f7c21f4ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_llm_function(model, prompt_template_function, postprocessor=None):\n",
    "    \n",
    "    def llm_function(*prompt_template_args, **kwargs):\n",
    "\n",
    "        prompt = prompt_template_function(*prompt_template_args)\n",
    "        response = model.generate(prompt, **kwargs)\n",
    "\n",
    "        if postprocessor:\n",
    "            response = postprocessor(response)\n",
    "\n",
    "        return response\n",
    "\n",
    "    return llm_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47524c-0397-4d55-ad0f-188bc0db8320",
   "metadata": {},
   "source": [
    "`make_llm_function` is a higher order function that will return an LLM function. As an improvement over our naive LLM functions above, LLM functions returned by `make_llm_function` can accept key word arguments to control LLM generation (`tokens_to_generate`, `top_k`, etc.), and also allows us to supply an optional `postprocessor` in cases where we want to postprocess model responses before returning them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a95ffd-a639-4f81-b9f9-f73ada89c961",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6bc8-9751-49b0-8213-f1847ab1af9a",
   "metadata": {},
   "source": [
    "## Exercise: Make Translation LLM Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9c926-c044-432b-aa4f-bb327622382a",
   "metadata": {},
   "source": [
    "For this exercise you are going to create a `translate` LLM function using the `make_llm_function` helper.\n",
    "\n",
    "You will want to use the following, as arguments to `make_llm_function`:\n",
    "1. An instance of GPT43B, provided just below as `llm`.\n",
    "2. Our prompt template for the translation task, provided below as `translation_template`.\n",
    "3. The postprocessing function `strip`, provided below will strip white space off the model's repsonse.\n",
    "\n",
    "Feel free to check out the *Solution* below if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab3a47-8d2c-4e5e-a4ba-938adb4e6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NemoServiceBaseModel(Models.gpt43b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802fc6d-dcc0-4087-b91e-583fad7febd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_template(text, source_language, target_language):\n",
    "    return f'Translate the following text from {source_language} to {target_language}:\\n{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc21dd-18a5-4a7c-8222-aa28e546fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(text):\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f0aff-4b0c-46ee-baec-bb6db0e28eb0",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d0cb9-b95f-49be-9d9f-f7167dca27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = 'TODO' # Make a `translate` LLM function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6865405-0619-4909-ae78-4eebf8f56ded",
   "metadata": {},
   "source": [
    "The following should work upon the successful implementation of `translate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec8638-0ed4-434b-82ff-34191e6af585",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('I am learning a lot.', 'English', 'Spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1aea53-2492-4022-af18-b0b66a4278b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42fcef-b37f-47e9-b6e2-2402edc10138",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = make_llm_function(llm, translation_template, postprocessor=strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc6140-39b0-429c-ace7-0c8ff154c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('I am learning a lot.', 'English', 'Spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd57ace-e658-48dd-83d9-c511a1b6caa0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bee974-458c-4949-88ea-9f5345f39d11",
   "metadata": {},
   "source": [
    "## Exercise: Make Fixed Language Translation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e288d7-b456-492e-a38d-43e07c30e6bc",
   "metadata": {},
   "source": [
    "The `translate` function you just created is very flexible and can translate to and from many languages. However, it's easy to imagine wanting a more specialized function that always translates from a specific language to a specific language. For this exercise you will create a new LLM function `translate_english_to_spanish` that will expect only a single argument, some English text, and will translate it to Spanish.\n",
    "\n",
    "In order to do this you will need to create a new prompt template, based on a modification of `postprocess_translation` above that only expects a single `text` argument.\n",
    "\n",
    "Feel free to check out the *Solution* below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638941e6-5ae5-44e0-97dc-f6df4df10365",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79868dbb-dff6-4d93-b77b-a40b90a11f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_english_to_spanish = 'TODO' # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde54ab3-9d01-4247-b66e-0c13b0c81647",
   "metadata": {},
   "source": [
    "The following should work upon the successful implementation of `translate_english_to_spanish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae9a0b-8506-4e9f-901a-143a471798aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_english_to_spanish('I am learning even more.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47408b2c-d159-4e93-9f9c-5922cface44b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1180c0-de36-44f6-83f4-551d8c673074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_spanish_translation_template(text):\n",
    "    return f'Translate the following text from English to Spanish:\\n{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7c567-c472-4167-8865-b7aca6ae86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_english_to_spanish = make_llm_function(llm, english_to_spanish_translation_template, postprocessor=strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712c0e1-4e0f-4735-8e59-9975cd3ce105",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_english_to_spanish('I am learning even more.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
