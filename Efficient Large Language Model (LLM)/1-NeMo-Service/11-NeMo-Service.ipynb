{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d3a1b9-dd8a-4598-9305-14e04c74c166",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4f436-6bb0-4a60-92e3-94e2a414aa79",
   "metadata": {},
   "source": [
    "# Working With NeMo Service Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c579e3-efb3-4ee7-bf10-a9ef1678d465",
   "metadata": {},
   "source": [
    "In this notebook you'll establish your first connection to NeMo Service, an NVIDIA cloud-hosted GPU service that we will be using throughout this workshop, and play around with doing text generation with some of the model's NeMo Service has to offer.\n",
    "\n",
    "NeMo Service has many incredible features, and we won't cover them all today, but in the context of LLM customization, NeMo Service will provide us with a variety of models that we can interface with through its API, as well as the ability to easily perform a variety of parameter-efficient fine-tuning techniques that we will exploit to great effect today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67f64f-1e2b-4eb3-af6f-f053ca3ae5f1",
   "metadata": {},
   "source": [
    "![NeMo Service](images/nemo_service.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2e2cd-534a-435e-bf97-860a4d52a877",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6b07a-2388-44b0-9e23-e5b63374a99d",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d45542-729d-4aa7-80b5-d4b68ff3c63c",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Know how to establish a connection to NeMo Service through its `nemollm.api` Python package.\n",
    "- Observe the variety of models available to us to use by way of the API.\n",
    "- Generate your first large language model responses for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6ea7a-2d74-48bc-b9a1-3d8573499743",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a3971-4baa-4af0-bd6b-b2d8b38f910f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf56cf-701b-4cab-bfe9-ecec4b58fa1f",
   "metadata": {},
   "source": [
    "We will begin every notebook by performing the imports necessary for the current notebook.\n",
    "\n",
    "Take note here of `from nemollm.api import NemoLLM` which imports the `NemoLLM` class that we will use ubiquitously in this course to communicate with NeMo Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807cfa5-a29a-4e8b-ac92-063655341429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nemollm.api import NemoLLM\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4ed0-a4f2-437d-9687-5fb85d197522",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815ecea-06fc-4ff5-9880-9abe7a0269b1",
   "metadata": {},
   "source": [
    "## Connect to LLM Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7005a4-d84e-4e97-ae3d-2aed3f6cc07a",
   "metadata": {},
   "source": [
    "Here is the boilerplate for establishing a connection to NeMo Service. For the workshop today we have provided an API key for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6c46a-32d3-4378-98c7-70d16595f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('NGC_API_KEY')\n",
    "api_host = os.getenv('API_HOST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44583f9-fca6-423f-87e9-99de2374c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = NemoLLM(\n",
    "    api_host=api_host,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8f602-18b0-40cd-846c-764c915ab316",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748e112-b975-4f4f-b694-ae92437c0f8c",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0e281-7bdb-48c5-bcf1-5c7fb076f4aa",
   "metadata": {},
   "source": [
    "NeMo Service hosts quite a few models out of the box and will also host the model customizations we create later in the workshop. Here is our first time using the `conn` object to make a call to the service with an API call. In this case we are requesting to see the base models available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbf27c-4b40-4c7b-8863-2bb0b441daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conn.list_models()\n",
    "models = {}\n",
    "\n",
    "for model in response['models']:\n",
    "    name = model.get('name')\n",
    "    features = model.get('features')\n",
    "    models[name] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c04f5e-0938-4ddd-909c-2817143eed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2cc86-f693-4f2e-8d41-848f4aa149e5",
   "metadata": {},
   "source": [
    "As you can see we have access to a variety of NeMo GPT models and also community models like LLaMA-2-70B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1300a75-6b7d-4265-8d03-b4c36f539f3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87e215-97cc-4b0c-9145-d344f5225524",
   "metadata": {},
   "source": [
    "## Model List Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39fe890-214d-4f2e-af5a-6cd33b00826a",
   "metadata": {},
   "source": [
    "This course contains a package `llm_utils`. In order to reduce boilerplate, and also as a reference to you for later work, `llm_utils` contains quite a lot of code that will be of service to you today. Throughout the course as you are introduced to code imported from `llm_utils`, you are encouraged to check out the imported modules in the `llm_utils` directory to learn more about how we approached working efficiently in the context of model customization.\n",
    "\n",
    "Our first imports are of enums we've created that we will use to make appropriate models available to us in specific notebooks. Each enum has a `list_models` method we will use to observe the models available to us. Here we list them all. You will see some overlap since some models are appropriate for use in multiple customization contexts.\n",
    "\n",
    "The `value` property (on the right-hand side, for example `gpt-43b-001`) is the actual string name that NeMo Service expects when we want to interact with a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe85e0d-b0dc-4464-9a5c-7813359c3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.models import Models, PtuneableModels, LoraModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a0fc8-2974-4eda-ab97-b60a174e8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef8446-b613-4782-8720-dc119ade4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PtuneableModels.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ca687-d7c3-4e09-bc85-142633948ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82055968-d05a-4d2f-9335-20b1ccf4355e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574906f-916b-4fdc-b918-a778a0adffb9",
   "metadata": {},
   "source": [
    "## Generating Model Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68f47b-5bd0-439f-a916-772db7211043",
   "metadata": {},
   "source": [
    "`conn.generate` is the method for sending prompts to NeMo Service LLMs and generating a response. As you can see from its docstring, it takes many mostly optional arguments to impact how the model generates a response. We will be introducing arguments to `generate` in the context of their use when appropriate in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc36b6e-52db-431a-be61-f3bc3df0f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(conn.generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b1545-d720-4803-974b-2d1f989626b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5591c-6d12-48bf-bef6-cbe94e983362",
   "metadata": {},
   "source": [
    "## Your First LLM Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5fbd8-25b1-4a80-977d-6fbb3b77552a",
   "metadata": {},
   "source": [
    "Here is the most basic possible way to generate a model response: pass `generate` a `model` name and a prompt. `conn.generate` will return a dict with details about the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89393044-81c8-487c-8604-110dae9c227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conn.generate(\n",
    "    model='gpt-43b-001',\n",
    "    prompt='Tell me about parameter efficient fine-tuning.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa638060-5ff2-4ed1-9bd4-facfdb4693ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364ec99-31e6-4337-ade8-df0df439379a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92354b-4f3d-4ed1-8629-eb9d69c1da5a",
   "metadata": {},
   "source": [
    "## Changing Model Response With Additional Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886e0a4-bb1e-4738-9e42-eaab3d346855",
   "metadata": {},
   "source": [
    "As you saw above, the response from `conn.generate` is by defaulta dict. In this course we will be almost entire focused on the quality of the text generated by the model and will prefer simple string outputs, which we can accomplish by setting `return_type='text'` as we do immediately below.\n",
    "\n",
    "In the following cell we also set `tokens_to_generate=100` which will influence the model to generate output of roughly 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa846b-5205-4452-b323-6bff1d128b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conn.generate(\n",
    "    model='gpt-43b-001',\n",
    "    prompt='Tell me about parameter efficient fine-tuning.',\n",
    "    tokens_to_generate=100,\n",
    "    return_type='text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d0acb-1357-4e08-91c9-ee65768c00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc04f4-d6f8-4fca-b250-2bad45857b11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61924d-7e24-462f-ba61-63530504cc1e",
   "metadata": {},
   "source": [
    "## NeMo Service Model Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921321a-e25b-4bcb-a6a7-0c14022322ed",
   "metadata": {},
   "source": [
    "Rather than work directly with `conn.generate` we are going to primarily interact with NeMo Service models through a helper class `NemoServiceBaseModel`. We've built several conveniences into this class that we will utilize at appropriate times throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63990d25-055d-40f8-ada4-77b7d13a5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.nemo_service_models import NemoServiceBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05bd3e-7c7c-4d92-826e-2915b513ebfa",
   "metadata": {},
   "source": [
    "To use the class we instantiate an instance with the NeMo Service model we would like to use. Here we select the LLaMA-2 70B chat variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18c0f8-9f18-4918-99a1-d74880d2f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NemoServiceBaseModel(Models.llama70b_chat.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8218bba-9d1e-49c4-9e19-2f8988f6d60f",
   "metadata": {},
   "source": [
    "`llm.generate` now behaves almost exactly like `conn.generate` except we don't need to pass the model name in every time we call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48548f2-c8f5-4d7f-b965-c25127de077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.generate('What is prompt engineering?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ab516-f795-4bcc-b6bf-97fa81cdca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca94f4-a121-4295-85a6-3e828dcd9454",
   "metadata": {},
   "source": [
    "During iterative prompt engineering it's nice to not have to wait for the entire response to be generated before viewing it. To accomplish this you can set `return_type='stream'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8f604-4021-45da-899e-eff952fa2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Tell me about large language models.', return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccf7a7-17ab-4499-96f0-6cbc030ef4a7",
   "metadata": {},
   "source": [
    "Just as a reminder, all the key word arguments available to `conn.generate` can also be passed into `llm.generate`. Next we will touch on several which you will likely want to use during the workshop, along with a couple other common techniques, like white space stripping, that are very common when working with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ff62b-320b-4bc2-893f-4ba789bc79ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e45e5f-f4eb-42b6-b933-3f11a032506e",
   "metadata": {},
   "source": [
    "## Tokens to Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865c69f-57d8-4956-89c1-a1020ad73cae",
   "metadata": {},
   "source": [
    "The `tokens_to_generate` named argument can control the maximum length of the model's response. Here we show a few examples of how changing its value can result in model responses of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a3f3d-5d3f-4194-abfb-2abdd07e594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Tell me about large language models.', tokens_to_generate=300, return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678baa2-c04d-4bb1-a79a-33ed80c4431f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff1f1f-a33d-426d-934d-924cc602bcc2",
   "metadata": {},
   "source": [
    "Here we drastically reduce the number of tokens to generate. Note that doing so doesn't mean that the model will \"complete its thoughts\" by the specified length, only that the generation will stop after this number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862f422-2601-4fcc-976a-f08c3f787f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Tell me about large language models.', tokens_to_generate=30, return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25a34a-0e9f-4b46-bba0-2df90a1e5207",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329eefc-30ff-4b07-a08e-3211f0e1f0f6",
   "metadata": {},
   "source": [
    "`tokens_to_generate` is especially helpful when we observe that a model is providing more of a response than we would like and we are only interested in the first part of its response.\n",
    "\n",
    "Here we give a toy example where we only want the model to give us a 'yes' or 'no' answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493c3e1-e6fc-443f-9e25-c4a064aeeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Is the Earth round?', tokens_to_generate=20, return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f65c1-66c7-49de-8ab8-44bc75fff41a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ae672-2d53-4b26-9f01-afa432b223d0",
   "metadata": {},
   "source": [
    "With `tokens_to_generate` we could capture only the part of the response we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbc29f-60c8-4908-bad2-930f5bf0ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Is the Earth round?', tokens_to_generate=5, return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed17514-72e1-4037-bda0-51eb560e6f7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d3f49-9369-49f6-a0ee-c8e7f26da31c",
   "metadata": {},
   "source": [
    "## White Space Stripping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904d8d4-ed59-4237-a5d9-ee1d4b0a5da8",
   "metadata": {},
   "source": [
    "You will almost always want to strip white space off your model's responses, which we can do with Python's `strip` method. Here is a simple LLM prompt, where you might notice some leading white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbcbe4-85e2-4df1-81f0-78ed663b281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('What is the capital of Califonia? Answer: ', tokens_to_generate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76463a-ef1e-4381-a8fe-3fd53f841ebc",
   "metadata": {},
   "source": [
    "Here we make the same call but use the Python string method `strip` to strip the extra, unwanted white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353be5a1-2e6c-41f7-9bd4-d1a08ccab93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('What is the capital of Califonia? Answer: ', tokens_to_generate=10).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ed835-6fbf-4323-a939-736073a3d206",
   "metadata": {},
   "source": [
    "It's worth pointing out that when we set `return_type='stream'` that we are unable to call Python string methods on the response since the streaming functionality is not retuning a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2d1a1-37f5-40f4-9885-b0f3b1b04fc4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77716fff-7931-4082-adf8-33f8477209ad",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781751f1-4a23-4827-8264-aacaaedd8982",
   "metadata": {},
   "source": [
    "Another very common technique with LLM responses is to want to stop generation given the presence of a specific token, typically a newline character `'\\n'` or some sentence-ending punctuation like a period `'.'`.\n",
    "\n",
    "Here is our basic text generation prompt from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c3e94-0905-4f67-9316-62d269840f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.generate('What is the capital of Califonia? Answer: ', tokens_to_generate=20).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd14bed-ae49-4ef2-b130-17171edc2809",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644626e-ff3f-433e-93f1-f2cc8119cab9",
   "metadata": {},
   "source": [
    "Here we will provide the `stop` named argument to `generate` to indicate the model should stop generating after the presence of a newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d2711-5229-44fb-bde4-f0ffe887f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.generate('What is the capital of Califonia? Answer: ', stop=['\\n'], tokens_to_generate=20).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665072e-443c-453d-96a3-4279a5f6e0c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4a3ae-6e91-40e4-a9aa-01d0d43f76ca",
   "metadata": {},
   "source": [
    "In this case we might accomplish something similar by stopping at periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f82fc5-42e6-404e-983d-f5a6321f0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.generate('What is the capital of Califonia? Answer: ', stop=['.'], tokens_to_generate=20).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002adb8-c941-482f-82e8-6a01dfb6fcca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a175e8b-92fe-4d82-bfd9-07fe7e32f63a",
   "metadata": {},
   "source": [
    "It's worth mentioning that `stop` expects a list of strings, so if you want, you can provide more that one stop character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a21142-fe15-4f47-852b-39201f27d6bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf6303-9785-4288-828c-1384d04efb13",
   "metadata": {},
   "source": [
    "## Controlling Model Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92004fad-2c42-4b17-90ee-c76c6ca0758b",
   "metadata": {},
   "source": [
    "The named arguments `top_k`, `temperature` and `top_p` can influence the randomness of a model's responses.\n",
    "\n",
    "Detailed coverage of these arguments is outside the scope of this workshop, but know that by default a model's response, given the same prompt, will be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886075c-0a0b-46c5-a1af-191b87197844",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Write a haiku. Haiku: ', return_type='stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd755e9-8d99-418d-8201-32af2d73854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Write a haiku. Haiku: ', return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846154d-fdd2-403c-b1c6-d9216c9bbb7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b69861-a815-4aa0-834c-e45a797f804e",
   "metadata": {},
   "source": [
    "By setting `top_k` an integer, to a value greater than 1 we can indicate that the model should consider more than the one most likely possibility for which token comes next.\n",
    "\n",
    "By setting `temperature`, a floating point value between 0 and 1, closer to 1, we can indicate that the model should even the probabilities of the possible next tokens.\n",
    "\n",
    "In this workshop, during sections on synthetic data generation, there will be times when you will likely want to increase `top_k` and `temperature` to create diverse outputs given the same prompt, as we do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bebe9d-388a-44ed-92bc-82dc410a722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Write a haiku. Haiku: ', top_k=3, temperature=.5, return_type='stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc9ef1-9bbe-4e71-9a5a-2a1ed48c7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate('Write a haiku. Haiku: ', top_k=3, temperature=.5, return_type='stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678b799-b6bc-4b61-9516-d6cb9355665e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64c4bd-6796-43c0-a0d7-50e00f64681b",
   "metadata": {},
   "source": [
    "## Warm Up Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea7fb2-01bb-4251-8019-0a4c2f06c088",
   "metadata": {},
   "source": [
    "You're going to be working with instances of `NemoServiceBaseModel` throughout the workshop and one of the main goals for this notebook is to get you comfortable working with it.\n",
    "\n",
    "To that end, before moving on to the next notebook, spend a few minutes trying out the following:\n",
    "- creating a new instance of `NemoServiceBaseModel` but this time choosing a different model.\n",
    "- Compare and contrast the output from the model you choose with that of the models we've already setup.\n",
    "- Try using some of the possible named arguments to `generate` like `tokens_to_generate`, `stop`, `top_k`, and `temperature` to see how it effects model generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce91675-6643-40a2-8926-0eabea9038c0",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c31c3-5558-4a16-b7e2-803280e0f92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
