{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7998bd03-c772-4a9a-b346-283949944b87",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c63f5-7a35-4601-85be-abf33740715a",
   "metadata": {},
   "source": [
    "# PubMedQA With Few-Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfdb7a-00ad-45f3-a4fe-0cb74661d56a",
   "metadata": {},
   "source": [
    "In this notebook we continue establishing a baseline on PubMedQA before performing PEFT, but this time with few-shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc685bc-3a87-4a39-b617-faee0f435a9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4bf73e-0635-4396-af32-f40fed6b52b2",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cc815-f276-4986-9d76-c2149031ecc0",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Format the PubMedQA data for few-shot prompting.\n",
    "- Evaluate few-shot performance for all 3 models across our entire test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c909cf6-d19e-4083-a8c1-0ff08feb70a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba54b5-a03e-4c3a-bcb0-8fa4da009535",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a57eb7-4e38-45f9-994c-414ea2be6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import PubmedModels\n",
    "from llm_utils.helpers import plot_experiment_results, accuracy_score\n",
    "from llm_utils.pubmedqa import generate_prompt_and_answer, strip_response\n",
    "from llm_utils.prompt_creators import create_prompt_with_examples, create_nemo_prompt_with_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699b808-1f8b-4a7b-90e2-39cd539036ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d976d9-fc7d-4a79-9709-de5a07ec1799",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf7cfe-331a-4baa-9168-637bd9c9f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "PubmedModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a4fbc-ce0e-4f45-be31-517bbbba9b8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4c669-036a-40bd-b84f-8a797e8f10df",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b61a4b-b0f3-45b8-b051-1534b8612ef6",
   "metadata": {},
   "source": [
    "We begin again by loading our `prompts_and_answers` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46845e36-429e-4120-ac4c-882b395475e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_answers = json.load(open('data/pubmedqa_panda_test.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d172d81-48d4-491a-9143-225ecc4144be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ea978-6966-4911-b5bd-2da71a5859a9",
   "metadata": {},
   "source": [
    "## Make Examples for Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82a620-fd43-42c4-8cbb-c7ee2f1e9b36",
   "metadata": {},
   "source": [
    "We are going to perform 3-shot prompting with one shot for each of the 3 possible responses. `pop_prompt_by_label` will find the first instance of a label, pop it out of the list and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658f242-b71e-47df-ac0d-f08a061a7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_prompt_by_label(prompts_list, label):\n",
    "    for i, (prompt, prompt_label) in enumerate(prompts_list):\n",
    "        if prompt_label == label:\n",
    "            return prompts_list.pop(i)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3f405-f0ad-4ef2-902d-6b6f3829b12d",
   "metadata": {},
   "source": [
    "### Check Data Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308f2da-81aa-4000-82a7-3626f32ffd2e",
   "metadata": {},
   "source": [
    "We start with 150 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b806644-6c96-4218-a0f9-9cfea321406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45734b16-aa08-4fae-b588-d6b72a111840",
   "metadata": {},
   "source": [
    "### Get Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210c60e-fd81-4000-93c2-780975f3c263",
   "metadata": {},
   "source": [
    "For each of the three possible labels we pop the first instance of it into an `examples` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebc5bb-d364-49ab-87c8-8f850d500a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for label in ['yes', 'no', 'maybe']:\n",
    "    example = pop_prompt_by_label(prompts_and_answers, label)\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9164c72-721a-452f-a329-4d3ad7a7d478",
   "metadata": {},
   "source": [
    "Here we confirm we have 3 example prompt and answer pairs, one for each of the 3 possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a21c3-5219-4390-91b8-854cc4905274",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12acde-c36a-475a-9d97-1153b556ef96",
   "metadata": {},
   "source": [
    "### Confirm Examples Have Been Removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275d3b9-6fcf-48c8-9a75-cfdfbd8fc465",
   "metadata": {},
   "source": [
    "We popped the examples out rather than copy them since we don't want to evaluate on the very prompts we are providing to the model for evaluation. Here we confirm `prompts_and_answers` has been reduced in length by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d2fac-81a0-4775-9287-60a3d86c9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af7ee2-9211-4b29-aa8f-ccbda0c03442",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8fcff-86fa-45a6-86b8-42de6438c7ae",
   "metadata": {},
   "source": [
    "## Generate Few-shot Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecf09034-0c8d-4c36-b85d-f4e9db949d0c",
   "metadata": {},
   "source": [
    "As a prerequisite for this workshop we assume your familiarity with few-shot prompting, including its use with instruction fine-tuned models which benefits from shots being formatted according to the template that was used for the model's instruction fine tuning. Feel free to ask an instructor or TA if you're not familiar with this technique and have questions about what we do in this section to properly format our few-shot promnpts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5684665-5d5d-44ce-b2b5-ca91f0bc2313",
   "metadata": {},
   "source": [
    "### NeMo Instruction Fine-tuned Prompt Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863697b-2ade-469d-a5a2-e0b967740afd",
   "metadata": {},
   "source": [
    "We will start with GPT43B which was instruction fine-tuned. To properly format example shots for this model we've provided the helper function `create_nemo_prompt_with_examples`.\n",
    "\n",
    "You'll notice a 3-shot prompt created by this helper function includes the addition of `User:` and `Assistant:` in various places to match GPT43B's instruction fine-tuning prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8747e-e6f2-4153-8df5-0abbeee983c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, _ in prompts_and_answers[:1]:\n",
    "    nemo_prompt_with_examples = create_nemo_prompt_with_examples(prompt, examples)\n",
    "    print(nemo_prompt_with_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947b4b4-0e11-462c-9298-976ab2316462",
   "metadata": {},
   "source": [
    "### Non-Instruction Fine-tuned Prompt Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29381427-e8bb-4d16-8b8e-c69e8e64aba8",
   "metadata": {},
   "source": [
    "GPT20B and GPT8B are not instruction fine-tuned models, and thus we only need to provide our example prompt/response shots in a straightforward manner. We've provided the `create_prompt_with_examples` helper function to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f2e61-6ddf-47bd-8971-5ab23f945995",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, _ in prompts_and_answers[:1]:\n",
    "    prompt_with_examples = create_prompt_with_examples(prompt, examples)\n",
    "    print(prompt_with_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b814f-304f-494d-920b-89138d3e923c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13024e-62e9-4fdf-97e4-29e013afc12d",
   "metadata": {},
   "source": [
    "## Try Few-shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af305f60-19bd-45fc-bce8-9dbfaf102075",
   "metadata": {},
   "source": [
    "As we did in the previous notebook, we will instantiate a model instance for 3 GPT models.\n",
    "\n",
    "`NemoServiceBaseModel` allows us to provide a function for creating prompts with examples, which will useful when performing few-shot inference with each of the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6dc7b-3ab0-4436-a736-55731f12f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d288d7-43f1-43fb-ad2a-7f3846984a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms['gpt8b'] = NemoServiceBaseModel(PubmedModels.gpt8b.value, create_prompt_with_examples=create_prompt_with_examples)\n",
    "llms['gpt20b'] = NemoServiceBaseModel(PubmedModels.gpt20b.value, create_prompt_with_examples=create_prompt_with_examples)\n",
    "llms['gpt43b'] = NemoServiceBaseModel(PubmedModels.gpt43b.value, create_prompt_with_examples=create_nemo_prompt_with_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105db9a1-ae0f-4d4e-9c32-ac95286457fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9055877-4280-4587-8a72-b5617c6eef86",
   "metadata": {},
   "source": [
    "Let's try the first 3 samples for each model using 3-shot prompting. You'll notice the use of `llm.create_prompts_with_examples` where each model instance will format the prompt appropriately for few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19a0b1-c05a-42bc-b5fc-977f5e7cefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, llm in llms.items():\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "    \n",
    "    for prompt, label in prompts_and_answers[:3]:\n",
    "        prompt_with_examples = llm.create_prompt_with_examples(prompt, examples)\n",
    "        response = llm.generate(prompt_with_examples, tokens_to_generate=1, return_type='text').strip()\n",
    "        print(f'Response from model: {response}')\n",
    "        print(f'Actual answer: {label}')\n",
    "        correct = label == response\n",
    "        print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d929320-1587-4f56-8cf5-87d008519ed7",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7ca97-cb8f-4583-8d04-b4ea1733c4c2",
   "metadata": {},
   "source": [
    "There are some interesting changes from our 0 shot prompting. While 8B still looks pretty bad we see at least one response that has the semblence of being correct. Meanwhile 20B looks to have improved a lot and 43B continues to go strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19a36c-897a-4a1c-831f-4bec48faa2b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad6c94-c4d8-4c5b-969c-81e0ff98bf88",
   "metadata": {},
   "source": [
    "## Format Data to Use Prompts With Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e1af7-388d-4281-a601-6104dce45dfe",
   "metadata": {},
   "source": [
    "If you recall, when using the `evaluate` method, we will be passing in a list of prompt/label pairs. To that end we need to format each prompt in `prompts_and_answers`, along with our `examples` list, into their approriate `prompt_with_examples` few-shot format.\n",
    "\n",
    "Here we do that twice, once for the instruction fine-tuned model, and once for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cd105-efe2-444f-81da-c8f3c4ec8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompts_and_answers = []\n",
    "nemo_few_shot_prompts_and_answers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d7483f-c973-491c-8b10-493f50a757f8",
   "metadata": {},
   "source": [
    "### Non-Instruction Fine-tuned Formatted Prompts With Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc391ac-85f8-4cc2-9c0a-f4d2ba7f54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers:\n",
    "    prompt_with_examples = create_prompt_with_examples(prompt, examples)\n",
    "    few_shot_prompts_and_answers.append((prompt_with_examples, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985bf94-4980-486e-9a97-5dc7372dac56",
   "metadata": {},
   "source": [
    "### NeMo GPT Formatted Prompts With Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216cc1e-b185-4ed4-a50c-ea593858e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers:\n",
    "    prompt_with_examples = create_nemo_prompt_with_examples(prompt, examples)\n",
    "    nemo_few_shot_prompts_and_answers.append((prompt_with_examples, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80956898-5614-45bd-b2cb-7cf3edb081bf",
   "metadata": {},
   "source": [
    "### Observe Shots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb15b51-b357-45de-a701-0d8fefbd2b6d",
   "metadata": {},
   "source": [
    "Let's make sure our few-shot prompts are formatted as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83328c-306a-4fcc-8ab7-15eea37f1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompts_and_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2462507-8553-43d9-a44c-9c78e16981b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_few_shot_prompts_and_answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee6ef3-7b75-4e17-ae67-e7ec56549246",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c2c57-0176-4e3b-a55c-5dbd0d45a3f0",
   "metadata": {},
   "source": [
    "## Evaluate Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8862a-6e98-4ac0-9d68-d2d19f04fbbe",
   "metadata": {},
   "source": [
    "Before evaluating on the whole test set and all 3 models, let's evaluate a small sample with one model as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17dd32-a471-4a29-b57c-ec6d803e5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b = llms['gpt43b']\n",
    "gpt43b.evaluate(nemo_few_shot_prompts_and_answers[:5], get_clean_prediction=strip_response, tokens_to_generate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c95eb-c72b-4d26-9565-955382a77b62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb37f87-a4ee-4d83-a1fd-451d4167116b",
   "metadata": {},
   "source": [
    "## Do Few-shot Prompts With Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c91c05-3c95-4738-bb65-535c3651f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, llm in tqdm(llms.items()):\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "\n",
    "    if name == 'gpt43b':\n",
    "        prompts_and_answers = nemo_few_shot_prompts_and_answers\n",
    "    elif name == 'gpt20b' or name == 'gpt8b':\n",
    "        prompts_and_answers = few_shot_prompts_and_answers\n",
    "    \n",
    "    llm.evaluate(prompts_and_answers[:10],\n",
    "                get_clean_prediction=strip_response,\n",
    "                write_results_to_csv=True,\n",
    "                experiment_name='Few Shot',\n",
    "                csv_file_name='experiment_results/pubmed_experiment_results.csv',\n",
    "                tokens_to_generate=1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c103518-9fc3-46db-b424-cbd4712d3f7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6017166-9fad-45e7-b68f-5b1f5893c397",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28525a1d-ba94-4e15-bff7-eb58d6dd4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_experiment_results('./experiment_results/pubmed_experiment_results.csv')\n",
    "# plot_experiment_results('./experiment_results/solutions/few_shot_pubmed_experiment_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9f245-9bc2-4336-935c-17185729fe55",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcad33-a31d-490b-a538-3c31a0be4cdf",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7fb70-7260-4547-b2e0-5dbdbc79600a",
   "metadata": {},
   "source": [
    "8B continues to not be up for the task at hand. 20B saw significant improvement with few-shot learning and 43B looks to have decreased in performance ever so slightly. This is interesting to notice and good to keep in mind, that while we have good guidelines to follow when it comes to customizing LLMs and their behavior, observation of empirical results is always critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
