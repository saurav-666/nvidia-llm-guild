{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e52959-0bac-4421-a95c-2934ea763eb5",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19837e82-458d-4ccb-b332-5d5231296f7e",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84677b-9042-44f4-abe9-43d64901e45b",
   "metadata": {},
   "source": [
    "In this notebook we explore the conceptual underpinnings behind the second major PEFT technique of this workshop: LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657ec09-e047-4fe4-84ec-c5ccf0960bdf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0874fcf-d8bd-445b-9a5f-dab93344005a",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339ca33-64ba-4003-803f-af8e7c8bce2d",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you:\n",
    "- Understand the structure and functionality of the LoRA PEFT technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b64ab-f3f3-425e-814e-1a4b4736399f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b834fd-86e7-44e6-8798-7410080d0df7",
   "metadata": {},
   "source": [
    "## LoRA Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a65d6e-4cb3-45f8-87a3-f3a572d45f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.slides import load_lora_slides\n",
    "load_lora_slides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0071c4-6bbc-4531-a40d-a89432b0df42",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd6f81-ad27-4aef-8ff0-43fb656aac78",
   "metadata": {},
   "source": [
    "## LoRA Simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834df3b-b664-4db1-997f-eb76018d96da",
   "metadata": {},
   "source": [
    " For the remainder of this notebook we will construct a simplified LoRA mechanism to help develop your intuition about how LoRA works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28139c-2513-4839-bfd9-d855eb7a33c6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3b8b8-d12c-4438-9127-97e1fbe2b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cda697-e33e-48ee-ad65-d67fdb4feeb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7276a6-f428-47c6-8946-867291215975",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20771c51-7c37-4853-ab98-3e06e0541738",
   "metadata": {},
   "source": [
    "Low-Rank Adaptation (**LoRA**) is a PEFT technique where we modify the weights of an  LLM using low-rank matrices. In LoRA, instead of directly altering the original weights of the LLM, we introduce trainable low-rank matrices that capture the desired adaptations.\n",
    "\n",
    "The key advantage of LoRA lies in its ability to make significant modifications to the LLM's behavior while training only a small number of additional parameters. This is achieved by decomposing the weight adjustments into lower-dimensional spaces using the introduced matrices. During the LoRA training process, the original weights of the LLM remain frozen, and only the parameters of the low-rank matrices are updated.\n",
    "\n",
    "This approach allows for substantial customization of the LLM's responses and capabilities without the computational burden of training the entire model. It is often said that we \"apply LoRA to an LLM\" or \"use a LoRA-modified LLM\", but it's important to note that the core LLM itself does not undergo retraining; rather, it's the additional low-rank matrices that are fine-tuned to achieve the desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214c95e-151b-4827-b9d8-ff462e43b60d",
   "metadata": {},
   "source": [
    "## Visual LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0d36f-86c7-4fc5-af6d-fb37e7ca0ed6",
   "metadata": {},
   "source": [
    "We will now build a toy implementation of LoRA. We will use the following image as a point of reference as we define the various components in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba21f0d-d1db-44f3-81e6-e45d3ce498a6",
   "metadata": {},
   "source": [
    "![LoRA](images/lora_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6486d7-2fe3-4223-a1e0-82a9587ebd89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a97e2-3620-4286-9fe0-812c7def0a4e",
   "metadata": {},
   "source": [
    "## LLM Weight Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0847c-3e96-4ea8-a94b-f8717bae7438",
   "metadata": {},
   "source": [
    "LLMs consist of thousands to tens of thousands **weight matrices**. In this simplified simulation we will treat the LLM as a single **weight matrix** (`W`) with small input and output dimensions `d` and `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f99e45-6fa8-4fa2-aa0d-2bce9da2bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 6  # Input dimension\n",
    "h = 8  # Output dimension\n",
    "W = np.random.randn(d, h)  # Weight matrix W with dimensions d x h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21692984-a6f0-4f24-8082-87106527e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748d4ea-c36b-4efa-beb4-0a4936b53662",
   "metadata": {},
   "source": [
    "For our mock LLM / small **weight matrix** the total number of tune-able parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfba79f-47c0-4887-9673-c6a81dd371f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbc392-7db8-4f23-938f-554a8613d9ab",
   "metadata": {},
   "source": [
    "![LoRA](images/lora_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f0046-cece-410a-95c7-b446741ce9df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f6b24-4b89-43a9-91fc-24131e64ccbc",
   "metadata": {},
   "source": [
    "## LoRA (Low Rank) Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba6090-09a7-44e2-8dac-736166c1961a",
   "metadata": {},
   "source": [
    "During LoRA PEFT we supply the **adapter dimension** or (low) rank of the **LoRA matrices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e3adb-06a0-41c1-88be-430a8996fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 2  # Rank for LoRA matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ebb09-bd45-4553-9748-1f4eb504d77e",
   "metadata": {},
   "source": [
    "For each **weight matrix** two **low-rank matrices** are created using the low-rank **adapter dimension** and the **weight matrix's** input and output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1678878-98c9-4141-acfe-4816abdd3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(r, d)  # Low-rank matrix A with dimensions (adapter dimension) x (weight matrice input dimension)\n",
    "B = np.random.randn(r, h)  # Low-rank matrix B with dimensions (adapter dimension) x (weight matrice output dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135aa5ef-98bc-4620-aa42-f796c34f7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bc3a7-413a-465b-aa4f-cad3d031035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348a042-8be2-4702-8bd6-69fb32848e14",
   "metadata": {},
   "source": [
    "The total number of tune-able parameters for the 2 **low-rank matrices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dbb92a-833d-490e-a080-09216b6e98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.size + B.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37adfe-ff75-4eb3-981a-1d6f31737a6a",
   "metadata": {},
   "source": [
    "Percentage of **low-rank matrix** tune-able parameters compared to the **weight-matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6297f99-3f12-4be5-924e-9e8a1bf130bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{(A.size + B.size) / W.size*100:.2f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8a13d-ee72-4ff0-9205-0c9f634b6d2d",
   "metadata": {},
   "source": [
    "![LoRA](images/lora_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35943cc7-b828-482d-9a2a-62023d91bc19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20342dd6-5210-483c-85a9-7f3ebd1464b5",
   "metadata": {},
   "source": [
    "## Low-Rank Factorization for Weight Matrix Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc9a51-c33b-4811-a54e-c106c8e45b09",
   "metadata": {},
   "source": [
    "The factorization of the **low-rank matrices**, when multiplied together, forms an approximation of the original high-dimensional **weight matrix**. The matrix product of `A` and `B`, referred to as the **approximation matrix**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf4a79-fb0e-4146-9119-1ebd84acd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = np.dot(A.T, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d6e1e-6ef6-4c68-8bff-7629abcd07e3",
   "metadata": {},
   "source": [
    "...represents the reduced representation of the original **weight matrix**, capturing its significant features with fewer parameters. The **approximation matrix** is the same size and shape as the **weight matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257f6a1-6c52-467c-babf-2ba2ad985d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87f8ef-265a-40ea-bbe4-b5fc5d22fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7262f-7e16-434f-aadf-47e0f915c67d",
   "metadata": {},
   "source": [
    "![LoRA](images/lora_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c2c00-d2b0-4f0f-80dc-5aeaa809b564",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687c682-0486-4a63-b505-202a273484eb",
   "metadata": {},
   "source": [
    "## Generating Output with Modified Weight Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e78381-cac2-4911-a24d-606aa7e5f3fb",
   "metadata": {},
   "source": [
    "The input and output vectors match the input dimension and output dimension of the **weight matrix** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f863f-3204-4b86-ac28-90cf46275dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.random.randn(d)  # Input vector of rank d\n",
    "output_vector = np.dot(input_vector, W)  # Output vector of rank h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213b2fc-72d5-44c7-afd3-25b57059d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f75d4-810a-4607-93e2-5b296149339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df1a7a-4361-4ca9-aa3c-678e901a7b3b",
   "metadata": {},
   "source": [
    "During inference, the **approximation matrix** is added to the **weight matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e55b1b-192e-4722-9269-ea08ccecb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_output_vector = np.dot(input_vector, W + AB)  # Output vector using modified W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a3721-dbb9-4b08-a61a-5ae214df2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_output_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25103bd4-1dfa-44c1-9cad-b18e8752cf50",
   "metadata": {},
   "source": [
    "![LoRA](images/lora_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971985-3ba7-4f13-bd8d-e1fc665c4a54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d0e8f-3267-44ca-9d83-06637106087e",
   "metadata": {},
   "source": [
    "## Exercise: Rerun With Larger Input and Output Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d5b10-92cb-4a89-9008-be145f43ea30",
   "metadata": {},
   "source": [
    "In this pass of the notebook we saw that the number of tune-able parameters of the **low-rank** matrices combined were only 58% of the those in the **weight matrix**. Go back to the top of the notebook and increase the size of the input and output dimensions and watch how the increase in size affects this percentage. You might try somewhat realistic values like\n",
    "- `d` = 1024 or 2048\n",
    "- `h` = 1024 or 2048\n",
    "- `r` = 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
