{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13467d12-0e4e-442d-b67f-73c0f89e8038",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b50c7-e8a4-4ee1-838d-554e8d0281dc",
   "metadata": {},
   "source": [
    "# PubMedQA With Zero-Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4e284-875a-4234-ba26-bb1098ca8ff7",
   "metadata": {},
   "source": [
    "In this notebook we'll obtain baseline performances for several of our available LLMs by performing and evaluating zero shot prompting on the PubMedQA question answering task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d43cf-6631-4062-813d-0a9e69a5442b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62a71f-32c5-4ee6-a8de-50f6dba53a1e",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39abe4-5145-427d-b2ac-69e4ef429186",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Format the raw PubMedQA data to be more suitable for prompting.\n",
    "- Evaluate zero-shot performance for 3 GPT models on PubMedQA Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c056d-e8df-4ca1-a5db-6dbc07f018aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a87aeb-b2bd-4f2e-bcbd-5b6dbc607737",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd69037-5e1b-48be-baf5-69505f45e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import PubmedModels\n",
    "from llm_utils.helpers import plot_experiment_results, accuracy_score\n",
    "from llm_utils.pubmedqa import strip_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39d57c-8f34-48d0-8077-727a63517253",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45fb3e-d39b-49ad-8ea8-ea60d636defe",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64b8be-3e19-4e28-a63e-7c92f025f4ae",
   "metadata": {},
   "source": [
    "While working with the PubMedQA data, we will be using several of the GPT models provided to us by NeMo Service, which we've collected for you in the `PubmedModels` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee9b1a-0a92-4310-a086-1d855323fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PubmedModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1036de-f290-42a1-9cf2-a745c75035fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe054cd-9674-4589-945e-b2e14b669eb4",
   "metadata": {},
   "source": [
    "## PubMedQA for Customization Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a937c-3464-42aa-98ea-f4ad818d1ee3",
   "metadata": {},
   "source": [
    "The PubMedQA question answering task will require our models to both perform reasoning using a lot of very specialized terminology, and also format responses in a very specific way. Additionally it is clear and well-labeled dataset. For these reasons it will serve us well as an entrypoint into PEFT, providing us not only the data needed for PEFT fine-tuning, but also as an opportunity to observe performance across a variety of models and customization techniques including zero and few-shot prompting, which will help us make a thorough quantitative analysis of how PEFT can benefit our use of these LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24def7d7-6ef9-4f9a-8c47-a2c8fb42c6b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03328482-b0fc-429e-acb8-d8570774fe8a",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2890c-b884-412a-abbd-26bb9fc53a13",
   "metadata": {},
   "source": [
    "Here we load the test split of the PubMedQA data created in the last notebook, which will give us a dictionary of 150 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f64ef-009c-4f1a-a1cc-e9c15e9e6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmedqa_data = json.load(open('data/pubmedqa_test.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd6e1c-f1f0-4cbe-a144-9ba646b56c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pubmedqa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf8f76-0833-4426-9103-a975b8212d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pubmedqa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2ca75-717e-4571-a952-ac984eda6553",
   "metadata": {},
   "source": [
    "Here we print the first entry in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502b631-1580-4747-8778-e9088310131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pubmedqa_data.values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b0337-6dc6-481b-ad0a-72d5e51f9946",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9bc86-c2a3-4ae0-b1c1-85dd9763216e",
   "metadata": {},
   "source": [
    "## Process Data in Prep for Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d1c4d-cd16-42bb-906b-385d0dfd78ea",
   "metadata": {},
   "source": [
    "Given the printout we just saw, let's reformat the data to be in a more suitable format for prompting. To do this we'll use `generate_prompts_with_answers` which for a given PubMedQA data entry will return a 2-tuple containing a prompt-formatted version of the entry, and its corresponding label.\n",
    "\n",
    "We've created this prompt format for you but please keep in mind as you go out to work with your own data that any time you need to convert data into a prompt for an LLM, time and care should be taken with the prompt engineering process to arrive at prompts that work well for your situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9479d1-f0f2-4eab-a833-03875cace5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts_with_answers(data):\n",
    "    prompt = \"\"\n",
    "    for index, context in enumerate(data['CONTEXTS']):\n",
    "        section_label = data['LABELS'][index]\n",
    "        prompt += f\"{section_label}: {context}\\n\"\n",
    "    \n",
    "    question_text = data['QUESTION']\n",
    "    prompt += f\"QUESTION: {question_text}\\n\"\n",
    "    prompt += f\"ANSWER (yes|no|maybe): \"\n",
    "\n",
    "    label = data['final_decision']\n",
    "    \n",
    "    return (prompt, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c056e04-1f96-4f4e-9565-a292876309c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_answers = []\n",
    "for value in pubmedqa_data.values():\n",
    "    prompts_and_answers.append(generate_prompts_with_answers(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c731972-73bc-41fd-897c-8ab3a1017edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ceed9f-50b8-4058-8aaa-0ad74e4d615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = prompts_and_answers[0][0]\n",
    "sample_answer = prompts_and_answers[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f434d6a-213f-4d39-8124-1d0b5e6970d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767f1c9-afa2-406b-befa-e9f7b3145f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b187a2-4670-4827-b54b-c0c1062fed26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d45d6-11ed-4349-9f78-61c1f995a6d4",
   "metadata": {},
   "source": [
    "## Write Formatted Prompts and Answers to File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2fc9a2-fc2f-453d-9a4e-4ef8f8f0e50d",
   "metadata": {},
   "source": [
    "We will be reusing `prompts_and_answers` in the next several notebooks so here we write it to file for easy re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6c6d6-d67e-42f6-a761-b3e71568e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pubmedqa_panda_test.json', 'w') as f:\n",
    "    json.dump(prompts_and_answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48479854-ba70-41f3-a344-d69c31205535",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b930ca5-0725-48df-8841-422fb09566c5",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa1e44-a759-463e-9a05-de3f53446151",
   "metadata": {},
   "source": [
    "We will begin our PubMedQA experiments with zero shot attempts using a GPT43B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a67be-5197-48ec-a3b6-8289fad103b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b = NemoServiceBaseModel(PubmedModels.gpt43b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739c35-ee9e-47ec-baa8-10b0ab31ec80",
   "metadata": {},
   "source": [
    "Because answers are expected to be `yes`, `no`, or `maybe` we limit token generation to 1 token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ca594-cd42-451d-8407-b07816422506",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = gpt43b.generate(prompt, tokens_to_generate=1)\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ab4e1-5197-4e4d-b450-240b2fe82f8f",
   "metadata": {},
   "source": [
    "We can see that the model did well with this small sample of answers in terms of generating the correct answer, however, it did insert some extra white space.\n",
    "\n",
    "Part of what we want to evaluate a fine-tuned model on is its ability to generate well-formatted responses, so with that in mind we do not, for the sake of these experiments, want to do a lot of post-processing. However, let's agree that stripping white space is almost always okay and do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc723fd6-3a12-4824-b390-c8e5f1be885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = gpt43b.generate(prompt, tokens_to_generate=1).strip() # Strip white space\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e5697-0543-4579-8e4b-be90a11ea352",
   "metadata": {},
   "source": [
    "At least with this small sample size, GPT43B appears to do very well with the PubMedQA task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1ebac-3522-4d6b-a136-26037face529",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580b22e-27dc-45da-881f-2aaffd94a402",
   "metadata": {},
   "source": [
    "## Experiment with Several GPT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6d8dd-12d2-40bf-bb5f-b9873a961d65",
   "metadata": {},
   "source": [
    "To get a better sample size of performance for our experiements, let's go ahead and utilize 3 different-sized GPT models. Here we create a dictionary `llms` containing all 3 of the GPT model instance types loaded into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad036a36-5e11-4902-86e5-eb3280a028e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = {model.name: NemoServiceBaseModel(model.value) for model in PubmedModels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b37638-6927-47be-b9d6-2c674ab9d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9a755-d6c0-4c24-aa45-1a4b168cf267",
   "metadata": {},
   "source": [
    "Now we will perform a sanity check for all of them on the first 3 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878bb913-5226-4137-a56c-f9d604ccf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, llm in llms.items():\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "    \n",
    "    for prompt, answer in prompts_and_answers[:3]:\n",
    "        response = llm.generate(prompt, tokens_to_generate=1).strip()\n",
    "        print(f'Response from model: {response}')\n",
    "        print(f'Actual answer: {answer}')\n",
    "        correct = response == answer\n",
    "        print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0abe48-c581-490e-991d-702d427b3031",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2864dd0-a95d-4433-bf2d-e27c820ee013",
   "metadata": {},
   "source": [
    "So far the 8B model gave us complete junk, the 20B model gave us somewhat well formatted but not always correct responses, and the 43B model appears to be doing great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb12bd-b490-47a2-a0a8-c8746238f6fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa29244-313a-4722-8213-3888dea7c8b3",
   "metadata": {},
   "source": [
    "## The NemoServiceBaseModel Evaluate Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed7a68-7724-459f-8d1b-849512d29146",
   "metadata": {},
   "source": [
    "Our `NemoServiceBaseModel` has an `evaluate` method we will be using. In its most simple form we can simply pass it a list of 2-tuples containing prompt/label pairs (like `prompts_and_answers`) and it will generate a response for every prompt, check the response against the label, and at the end of its run print an accuracy score for the run.\n",
    "\n",
    "Since evaluate is calling `generate` under the hood, we can also pass in other generation parameters like `tokens_to_generate`, `top_k`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff335d81-626a-4c28-8ee7-97cd7027e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b.evaluate(prompts_and_answers[:3], tokens_to_generate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5971a5-2091-4f71-8a60-db01e5c7e991",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1ff94-21b9-4909-8d5a-e510cd732ecf",
   "metadata": {},
   "source": [
    "## Post Processing With Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf8cac-2fba-40cc-8948-1aa55848a825",
   "metadata": {},
   "source": [
    "By default `evaluate` will not perform any post processing on the model output. With what we observed above, it makes sense that `gpt43b` scored 0 on the first 2 samples since we know it is including some white space at the beginning of the response.\n",
    "\n",
    "If we'd like we can pass a `get_clean_prediction` function to `evaluate` which should expect a model response string, perform some post-processing on it, and return the results which are then compared to the provided label.\n",
    "\n",
    "For our case let's use the following `strip_response` function to trim white space off the responses before comparing them to their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fad6f9-cee9-47d1-b1c8-0477c930c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_response(response: str) -> str:\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46d940-5035-4b81-a99e-2d014962543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b.evaluate(prompts_and_answers[:3], get_clean_prediction=strip_response, tokens_to_generate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f81136-cc70-403e-b69b-bd78b63549bb",
   "metadata": {},
   "source": [
    "Having stripped off the white space we see the GPT43B model performing as well as we observed above.\n",
    "\n",
    "It's worth mentioning that the intended use of `evaluate` is to evaluate model performance across datasets and is not a replacement for `generate`. If you need to view individual model responses for any reason, switch to using `generate` on the data samples in question so you can see model output directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cf045-b2c4-43f3-ae6f-b6e96eb1cf31",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfde1d-98c8-47e5-a03c-ea390a7b9479",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting With Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb51cf0-7df5-4e0e-ba5a-df1ebad3cbbe",
   "metadata": {},
   "source": [
    "Now we will iterate through all 3 GPT models and evaluate their performance on our test set. In doing so we will also introduce another feature of the `generate` method which is to write evaluation results to a csv file. Later we will visualize the results to better observe how each of the models perform in different prompting and fine-tuning scenarios.\n",
    "\n",
    "We will be writing to `'experiment_results/pubmed_experiment_results.csv'`. This file will contain results for all of our experiments with the PubMedQA data including zero, one and few-shot prompting, p-tuning and LoRA, with a variety of models.\n",
    "\n",
    "The model instance itself will provide the model name to the experiment results file, so we don't need to pass that in manually. We supply `experiment_name` to specify the customization conditions for this particular run, in our case, \"Zero Shot\".\n",
    "\n",
    "The following, which runs the whole `prompts_and_answers` dataset through 3 GPT models will take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df124ca-b984-4c04-b860-198bb4f94d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, llm in tqdm(llms.items()):\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "    \n",
    "    llm.evaluate(prompts_and_answers,\n",
    "                get_clean_prediction=strip_response,\n",
    "                write_results_to_csv=True,\n",
    "                experiment_name='Zero Shot',\n",
    "                csv_file_name='experiment_results/pubmed_experiment_results.csv',\n",
    "                tokens_to_generate=1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71fd00-a326-404d-a2a4-ae6fea0e18d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dd301-cd83-4b7f-b4f7-e7c98e5a052e",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac76d1-f99f-4508-816d-05686d683b75",
   "metadata": {},
   "source": [
    "In addition to the printouts above, each model instance has an `experiment_results` property we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce68d17-e657-4e72-b6f0-54dd51f205fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in llms.values():\n",
    "    print(llm.experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538e348-8200-4742-b745-52a03b8e1715",
   "metadata": {},
   "source": [
    "Additionally, we have provided a `plot_experiment_results` helper for better visualization, especially when we start to conduct multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d3af3-ae5a-4a02-9e24-53c07c7d6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_experiment_results('experiment_results/pubmed_experiment_results.csv')\n",
    "\n",
    "# We also provide solution/reference results in case they are needed.\n",
    "# plot_experiment_results('experiment_results/solutions/zero_shot_pubmed_experiment_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b2cb0-04ed-4517-a62d-6a481a410035",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20b082-84c0-43d2-9b12-7624501fca3c",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562ac87-c8f1-4682-a4d1-e75306ec4823",
   "metadata": {},
   "source": [
    "Much like we observed above with the small sample size, the model's have performed to varying degrees, largely relative to their size. 43B appears to be doing quite well, 20B didn't fail entirely, but only got some responses correct, and 8B didn't get a single response correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
