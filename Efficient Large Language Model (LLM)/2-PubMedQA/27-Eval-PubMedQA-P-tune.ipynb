{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a12303b-689b-4052-9561-6fd5e96d6dfe",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e59fe-18bf-4d46-9bf5-a0613080df78",
   "metadata": {},
   "source": [
    "# Evaluate P-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fda0e-86e0-4e29-b158-15400b3c3166",
   "metadata": {},
   "source": [
    "In this notebook we evaluate the performance of the 3 p-tuned GPT models on the PubMedQA question answering task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c3c5d-0c26-4ac2-a250-f1cb2b71ec01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee99794-a8ce-401d-9aac-9020af1101e6",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343e7aa-b256-4b1b-89e0-db72d2bf7249",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Evaluate p-tuning performance for all 3 models across our entire test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536c23c-283d-4ca9-80ef-dbad18f6d912",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c0840-ad76-4b0c-91a9-d1b5455be925",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd69037-5e1b-48be-baf5-69505f45e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import PtuneableModels\n",
    "from llm_utils.helpers import plot_experiment_results, accuracy_score\n",
    "from llm_utils.pubmedqa import generate_prompt_and_answer, strip_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4875cd2-7365-45d4-a97c-2f7dcd6735d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727b9b1-5719-4f05-b184-aaa8ea5fafd9",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70a298-0da1-4a2f-88ba-fe550892414b",
   "metadata": {},
   "source": [
    "In this notebook we import models that support p-tuning. Of note is that the GPT43B model is a slightly different variant than the GPT43B model we have been using (`gpt-43b-001`) and that LLaMA-2-70B (not the chat variant) is also available for p-tuning, although we are not using it to the end in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf7cfe-331a-4baa-9168-637bd9c9f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "PtuneableModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4aa292-6317-4715-8e2a-8f6d518e15dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed50e6-7354-4fb1-84aa-3b19a9aa7633",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddf289-663e-4f6f-b696-390757eeb01e",
   "metadata": {},
   "source": [
    "We begin by loading our `prompts_and_answers` test data created in a previous notebook from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46845e36-429e-4120-ac4c-882b395475e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_answers = json.load(open('data/pubmedqa_panda_test.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ec0cd-ec6c-4440-8e45-1e8be172277b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd1c61-5d76-4b77-88d6-16642ba4e4b2",
   "metadata": {},
   "source": [
    "## One Model, Many Customizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f509f-f083-4305-87ea-efd059ca0b07",
   "metadata": {},
   "source": [
    "One of the big plusses of p-tuning is that the customization lives in the small prompt encoder network we trained during p-tuning. This means that we can simply swap the prompt encoder in and out when we want to use the customization vs. when we do not.\n",
    "\n",
    "Additionally, if we have multiple p-tuning customizations that were performed on the same model, we can use a single instance of the LLM and simply use a variety of p-tuned prompt encoders to enable several kinds of customized behavior without the need to host multiple instances of the LLM.\n",
    "\n",
    "In the case of NeMo Service, the service is taking care of these steps on our behalf, which is lovely. If you're performing PEFT customizations with other hosted services you may find the same.\n",
    "\n",
    "If you are working with your own local models outside the context of a hosted service, say with [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/llama2peft.html), [huggingface/peft](https://huggingface.co/blog/peft), or your own PyTorch setup, you would need to manage this yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea8ec7-5c60-40e4-9ba7-41c008bff885",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516f365-b748-4802-9d16-a566ee62bba1",
   "metadata": {},
   "source": [
    "## Using a Customization With NeMo Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272052b4-0822-49bd-ae4b-be3b04830ae5",
   "metadata": {},
   "source": [
    "To use a p-tuned model with the NeMo Service we simply need to obtain the `customization_id` obtained after performing the customization. Once we have it, we can include it in calls to the base model that was p-tuned. NeMo Service takes care of managing the LLMs and customization on our behalf.\n",
    "\n",
    "Typically you would either use `conn.list_customizations` or the web GUI to ascertain your p-tuned models' `customization_id`s, but here we will provide them for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39a46c-9689-4346-a3c8-995b27dbac24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df923ae8-e831-48f2-85e8-19edb133693e",
   "metadata": {},
   "source": [
    "## Instantiate P-tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae90966-9ad6-42fe-9c0a-7b1d28805537",
   "metadata": {},
   "source": [
    "Again we will instantiate instances of 3 NeMo GPT models, but this time passing the appropriate `customization_id` to each. With NeMo Service this is all we have to do to use a model customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afc07c-d747-45ba-a216-18f347733241",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c655d-3d77-4f1d-9530-d37fa3dcf0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms['gpt43b'] = NemoServiceBaseModel(model=PtuneableModels.gpt43b.value, customization_id='47fadeed-746b-4971-8d48-1cd0980d5ddc')\n",
    "llms['gpt20b'] = NemoServiceBaseModel(model=PtuneableModels.gpt20b.value, customization_id='a2b8960b-a31b-4fe5-9bdf-32f95e751c47')\n",
    "llms['gpt8b'] = NemoServiceBaseModel(model=PtuneableModels.gpt8b.value, customization_id='1780214f-8582-4e1c-89cd-7488f089e167')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde45af-31eb-4a08-9367-87828369da4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4d8d4-58fc-4e71-9870-6d387fcab0c3",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompts With P-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74abc1a-8a4a-488b-b306-413027f7426a",
   "metadata": {},
   "source": [
    "We will now try out each of our customized models on a few samples of the PubMedQA test data, just as we did in the zero and few-shot notebooks earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059c422-e460-4410-9311-5356aca21140",
   "metadata": {},
   "source": [
    "### GPT43B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad238e62-392a-4acb-9195-a21f7c456451",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in tqdm(prompts_and_answers[:3]):\n",
    "    response = llms['gpt43b'].generate(prompt, tokens_to_generate=1).strip()\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54191059-3ea5-4f5a-82fa-2569dc051f8d",
   "metadata": {},
   "source": [
    "### GPT-20B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3a1f9-211f-4daa-a976-e5f49e19f32c",
   "metadata": {},
   "source": [
    "Now let's check the 20B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4b977-415a-4261-b95b-cc453c1467fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = llms['gpt20b'].generate(prompt, tokens_to_generate=1).strip()\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5d331-d124-4e95-8077-4dfb5cd0bc9a",
   "metadata": {},
   "source": [
    "Strangely it looks like we are getting empty responses. Let's increase the number of tokens generated to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8eade6-e0f1-4139-9a24-87bc63cfc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = llms['gpt20b'].generate(prompt, tokens_to_generate=2).strip()\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85c4b1-8093-4d0b-b744-0c1fdbaecf26",
   "metadata": {},
   "source": [
    "For reasons we don't yet understand, it looks like generating 2 tokens results in a sensible output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e0285-9a45-4245-9e80-d1a299429d26",
   "metadata": {},
   "source": [
    "### GPT-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7da8b-269c-4179-9e38-284af660ec02",
   "metadata": {},
   "source": [
    "Now let's check the 8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4bdcc-d369-4a81-8f4b-baefa7e7b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = llms['gpt8b'].generate(prompt, tokens_to_generate=1).strip()\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0d28e-1717-478e-aed9-c2d7f30c7349",
   "metadata": {},
   "source": [
    "At least in our sample, it hasn't gotten any answers correct, but at least for the most part its responses are looking far better than before p-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da630ac-6813-4070-9e18-80d43ee2a22f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b88461-3814-4e36-994b-0e004310d3aa",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting With P-tuned Model on Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb567d-089f-420b-9466-0e7b7679c05d",
   "metadata": {},
   "source": [
    "Now will run evaluate each of the three customized models on the full test data set.\n",
    "\n",
    "Due to our observations about GPT20B needing 2 tokens to generate a correct response, we'll take care to handle that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669b133-bd13-4ebf-935b-b00e753d6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, llm in tqdm(llms.items()):\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "\n",
    "    # Handle 20B needing 2 tokens\n",
    "    tokens_to_generate = 2 if name == 'gpt20b' else 1\n",
    "    # 43B p-tuning model is different than previous 43B.\n",
    "    # Here we give it a model_description to match previous\n",
    "    # experiments just for better plotting.\n",
    "    model_description = 'gpt-43b-001' if name == 'gpt43b' else ''\n",
    "    \n",
    "    llm.evaluate(prompts_and_answers[:10],\n",
    "                get_clean_prediction=strip_response,\n",
    "                write_results_to_csv=True,\n",
    "                experiment_name='P-tuned',\n",
    "                csv_file_name='./experiment_results/pubmed_experiment_results.csv',\n",
    "                model_description=model_description,\n",
    "                tokens_to_generate=tokens_to_generate)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce68d17-e657-4e72-b6f0-54dd51f205fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in llms.values():\n",
    "    print(llm.experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b75376-0cb1-4491-ad92-521a10dea1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_experiment_results('./experiment_results/pubmed_experiment_results.csv')\n",
    "plot_experiment_results('./experiment_results/solutions/ptune_pubmed_experiment_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226af2a4-1132-42c0-9dd7-324ab72e3ced",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f5fee-1ba6-4931-b5bf-6ae0be547d64",
   "metadata": {},
   "source": [
    "For all 3 models, p-tuning over 3 epochs on 700 samples gave us better performance than prompt engineering alone.\n",
    "\n",
    "For 43B the performance was only slightly improved over one-shot learning. In this experiement 43B had the least to gain from p-tuning.\n",
    "\n",
    "20B performed marginally better than with 3-shot learning, but drastically better than with zero-shot learning. If you recall just how long our 3-shot prompts were, the excellent news here for 20B is that we can improve performance while simultaneously and drastically reducing the number of tokens we need to send to the model. Not only can this result in better performance, but if we happen to be in a scenario where there is a cost to number of tokens sent, then over time we could really save a lot.\n",
    "\n",
    "8B is still doing quite poorly (poor little guy ðŸ‘¼) but we should be pleased that for the first time it's not batting 0. Perhaps with additional training time, or data more specifically crafted for this model we could do even better. Additionally, given our observations above, a little additional post-processing, specifically using `.lower()` on our responses, would likely give us a drastic increase in accuracty. We will return to 8B's performance on PubMedQA when after we turn our attention to LoRA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
